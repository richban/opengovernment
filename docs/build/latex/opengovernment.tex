% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,12pt,oneside]{sphinxmanual}

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
\else\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage[english]{babel}
\usepackage{times}
\usepackage[Sonny]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}


\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{2}


\title{Design and implementation of open-data data warehouse Documentation}
\date{May 29, 2016}
\release{0.0.1}
\author{Richard Banyi}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}

\listoffigures
\listoftables

\chapter{Introduction}
\label{introduction::doc}\label{introduction:welcome-to-design-and-implementation-of-open-data-data-warehouse-s-documentation}\label{introduction:introduction}

\section{Data}
\label{introduction:data}

\subsection{What is Data?}
\label{introduction:what-is-data}
Data is all around us. But what exactly is? Data is a value assigned to a thing.
What can we say about the balls in the picture? They are
tennis balls, right? So one of the first data points we have is that they are used for tennis.
Tennis is a category sport, so this helps us to to put the balls in a taxonomy.
But there is more to them. We have the colour: “green”, the condition “new”.
They all have sizes. All kind of objects have lot of data attached to them.
Even people do: they have a name a date of birth, weight, height etc.
All these things are data \footnote[1]{\sphinxAtStartFootnote%
Author: School of Data
Date: Sep 02, 2013
Organization: School of Data
Available: \href{http://schoolofdata.org/handbook/courses/what-is-data/}{Data Fundamentals}
} \titleref{Strunk1979}.
\begin{figure}[h]
\centering
\capstart

\scalebox{0.500000}{\includegraphics{{tennis}.jpg}}
\caption{Tennis Balls}\label{introduction:id15}\end{figure}

\textbf{Qualitative} data is everything that refers to the quality of something.
A description of experiences, a description of colours, and interview are all qualitative data.
Data that can be observed but not measured.

\textbf{Quantitative} data is data that refers to a number. Data that can be measured. E.g. the number of tennis balls, the size, a score on a test etc.

\textbf{Categorical} data puts the item you are describing into a category. For example the condition “new”, “used”, “broken” etc.

\textbf{Discrete data} is a numerical data that has gaps in it: e.g. the count of golf balls. There can only be whole numbers of tennis balls, there is no such thins as 0.5 golf balls.

\textbf{Continues} data is a numerical data with a continues range: e.g. size of a tennis balls can be any size (86.02mm), or the size of your foot (as opposed to your shoe size, which is discrete). In continues data, all values are possible with no gaps between .


\subsection{From Data to information knowledge}
\label{introduction:from-data-to-information-knowledge}
Data, when collected and structured suddenly becomes a lot more useful.

\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textsf{\relax 
Category
} & \textsf{\relax 
Contract
}\\
\hline
Date
 & 
2015
\\
\hline
Amount
 & 
\$1232.21
\\
\hline
Recipient
 & 
Apple Inc.
\\
\hline\end{tabulary}


\textbf{Data}: Content that is directly observable or verifiable; a fact - it’s -5C outside.

\textbf{Information:} Content that represents analyzed data - “it’s -5C outside I’ll take a warm coat''.

\textbf{Knowledge:} “I remember the last time when was this cold I got a cold. I’ll therefore take a scarf and gloves.
But first I’ll check with Brian. He usually dress too light for this kind of weather.”
Knowledge is created when the information is learned, applied and understood.

\textbf{Context:} One thing incredibly important for data is context: A number or quality doesn’t mean a thing if you don’t give context.
So explain what you are showing – explain how it is read, explain where the data comes from and explain what you did with it.
If you give the proper context the conclusion should come right out of the data.


\subsection{Finding Data}
\label{introduction:finding-data}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textsf{\relax 
\textbf{Data Source}
} & \textsf{\relax 
\textbf{Description}
}\\
\hline
csv
 & 
Comma Separated values (CSV)
\\
\hline
xls
 & 
MS Excel Spreadsheet
\\
\hline
gdoc
 & 
Relational database table
\\
\hline
mongodb
 & 
MongoDB database
\\
\hline\end{tabulary}


There are three basic ways of getting hold of data:
\begin{enumerate}
\item {} 
\textbf{Finding data} - involves searching and finding data that has been already released

\item {} 
\textbf{Getting hold of more data} - asking for “new” data from official sources e.g. trough
Freedom of Information requests. Sometimes data is public on a website but there is not
a download link to get hold of it in bulk! This data can be liberated with what
datawranglers call scraping.

\item {} 
\textbf{Collecting data yourself} - gathering data and entering it into a database or a
spreadsheet.

\end{enumerate}


\subsection{Identify data source}
\label{introduction:identify-data-source}
In recent years \emph{governments} have begun to release some of their data to the public - open data.
Many governments host special (open) government data platforms for the data they create.
For example the UK government started \href{https://www.ukdataservice.ac.uk}{UK Data}, or USA \href{http://www.data.gov}{data.gov}.
Other sources of data are large \emph{organisations}.
The World Bank and the World Health Organization for example regularly release reports and data sets.
Scientific projects and institutions release data to the scientific community and the general public.
Open data is produced by \href{https://www.nasa.gov}{NASA} for example, and many specific disciplines have their own data repositories, some of which are open.


\subsection{Problem with sources}
\label{introduction:problem-with-sources}
There are plenty of places where you can get hand on open datasets which are open to public, however these
sources are often unstructered, very messy, ambiguous, mis-use of class attributes, non-consistent, missing values,
etc. The unstructured data growing quickiest than the other, and their exploitation could help in business decision.

\def\SphinxLiteralBlockLabel{\label{introduction:example-of-unstructed-data}}
\begin{Verbatim}[commandchars=\\\{\}]
5896 34513500 34513500\PYGZhy{}1     \PYGZbs{}N      \PYGZbs{}N Transport equipment
3031 48825000 48825000\PYGZhy{}7     \PYGZbs{}N      \PYGZbs{}N      Software package
5821 34144900 34144900\PYGZhy{}7     1.9721673495    \PYGZbs{}N      Transport equipment
\end{Verbatim}
\let\SphinxLiteralBlockLabel\empty


\section{Open Data}
\label{introduction:open-data}
Do you know exactly how much of your tax money is spent on street lights or on public transportation? And what is in the air that you breathe along the way? Where in your region will you find the best job opportunities and the highest number of fruit trees per
capita? When can you influence decisions about topics you deeply care about, and whom should you talk to?

New technologies now make it possible to build the services to answer these questions automatically. Much of the data you would need to answer these questions is generated by public bodies. However, often the data required is not yet available in a form which is easy to use - take for example our country Slovakia it still lack of data transparency and creating data sets which can be easy to used.

The notion of open data and specifically open government data - information, public or otherwise, which anyone is free to access and re-use for any purpose - has been around for some years.


\subsection{Why open Data?}
\label{introduction:why-open-data}
Open data, especially open government data, is a tremendous resource that is as yet largely untapped. Many individuals and organisations collect a broad range of different types of data in order to perform their tasks. Government is particularly significant in this respect, both because of the quantity and centrality of the data it collects, but also because most of that government data is public data by law, and therefore could be made open and made available for others to use. Why is that of interest?

There are also many different groups of people and organisations who can benefit from the availability of open data, including government itself. At the same time it is impossible to predict precisely how and where value will be created in the future.

It is already possible to point to a large number of areas where open government data is creating value. Some of these areas include:
\begin{itemize}
\item {} 
Transparency and democratic control

\item {} 
Participation

\item {} 
Self-empowerment

\item {} 
Improved or new private products and services

\item {} 
Innovation

\item {} 
Improved efficiency of government services

\item {} 
Improved effectiveness of government services

\item {} 
Impact measurement of policies

\item {} 
New knowledge from combined data sources and patterns in large data volumes

\end{itemize}

Open government data can also help you to make better decisions in your own life, or enable you to be more active in society. A woman in Denmark built \href{http://findtoilet.dk}{findtoilet.dk}, which showed all the Danish public toilets, so that people she knew with bladder problems can now trust themselves to go out more again.
Services like ‘mapumental’ in the UK and ‘mapnificent’ in Germany allow you to find places to live, taking into account the duration of your commute to work, housing prices, and how beautiful an area is. All these examples use open government data.

Open data is also of value for government itself. For example, it can increase government efficiency. The Dutch Ministry of Education has published all of their education-related data online for re-use. Since then, the number of questions they receive has dropped, reducing work-load and costs, and the remaining questions are now also easier for civil servants to answer, because it is clear where the relevant data can be found. Open data is also making government more effective, which ultimately also reduces costs.

While there are numerous instances of the ways in which open data is already creating both social and economic value, we don’t yet know what new things will become possible. New combinations of data can create new knowledge and insights, which can lead to whole new fields of application. We have seen this in the past, for example when Dr. Snow discovered the relationship between drinking water pollution and cholera in London in the 19th century, by combining data about cholera deaths with the location of water wells.

This untapped potential can be unleashed if we turn public government data into open data. This will only happen, however, if it is really open, i.e. if there are no restrictions (legal, financial or technological) to its re-use by others. Every restriction will exclude people from re-using the public data, and make it harder to find valuable ways of doing that. For the potential to be realized, public data needs to be open data.


\subsection{What is Open Data?}
\label{introduction:what-is-open-data}
Open data is data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and sharealike \footnote[3]{\sphinxAtStartFootnote%
Open Definition \href{http://opendefinition.org}{see}
}.

The full Open Definition gives precise details as to what this means. To summarize the most important:
\begin{itemize}
\item {} 
\textbf{Availability and Access:} the data must be available as a whole and at no more than a reasonable reproduction cost, preferably by downloading over the internet. The data must also be available in a convenient and modifiable form.

\item {} 
\textbf{Re-use and Redistribution:} the data must be provided under terms that permit re-use and redistribution including the intermixing with other datasets.

\item {} 
\textbf{Universal Participation:} everyone must be able to use, re-use and redistribute - there should be no discrimination against fields of endeavour or against persons or groups. For example, ‘non-commercial’ restrictions that would prevent ‘commercial’ use, or restrictions of use for certain purposes (e.g. only in education), are not allowed.

\end{itemize}

If you’re wondering why it is so important to be clear about what open means and why this definition is used, there’s a simple answer: \textbf{interoperability}.

Interoperability denotes the ability of diverse systems and organizations to work together (inter-operate). In this case, it is the ability to interoperate - or intermix - different datasets.

Interoperability is important because it allows for different components to work together. This ability to componentize and to ‘plug together’ components is essential to building large, complex systems. Without interoperability this becomes near impossible — as evidenced in the most famous myth of the Tower of Babel where the (in)ability to communicate (to interoperate) resulted in the complete breakdown of the tower-building effort.
We face a similar situation with regard to data. The core of a “commons” of data (or code) is that one piece of “open” material contained therein can be freely intermixed with other “open” material. This interoperability is absolutely key to realizing the main practical benefits of “openness”: the dramatically enhanced ability to combine different datasets together and thereby to develop more and better products and services.

Providing a clear definition of openness ensures that when you get two open datasets from two different sources, you will be able to combine them together, and it ensures that
we avoid our own ‘tower of babel’: lots of datasets but little or no ability to combine them together into the larger systems where the real value lies \footnote[4]{\sphinxAtStartFootnote%
Open Knowledge \href{http://opendatahandbook.org/guide/en/what-is-open-data/}{Open Data Handbook}
}.


\section{Data Warehouse Fundamentals}
\label{introduction:data-warehouse-fundamentals}

\subsection{What is Data Warehouse}
\label{introduction:what-is-data-warehouse}
A data warehouse (DW or DWH) is a system used for reporting and data analysis.
Data Warehouse's are central repositories of integrated data from one or more disparate sources.
They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analyses.

\emph{A data warehouse is a system that extracts, cleans, conforms, and delivers source data into a dimensional data store and then supports and implements querying and analysis for the purpose of decision making} \footnote[5]{\sphinxAtStartFootnote%
The Data Warehouse ETL Toolkit, Ralph Kimball, Joe Casetra, Copyright 2004 by Wiley Publishing, Inc. All rights reserved., eISBN: 0-764-57923-1
}.


\subsection{Operational Systems}
\label{introduction:operational-systems}
An operational system directly supports the execution of a business process.
By capturing details about significant events or transactions.
A sales system, for example captures information about orders, shipments, and returns.

Operational systems must enable several types of database interaction, including inserts, updates,
and deletes - these interactions are almost always atomic.
For example, an order entry system must provide for the management of lists of products, customers,
and salespeople; the entering of orders; the printing of order summaries, invoices, and packing
lists; and the tracking order status. The operational system is likely to update as things change
(if a customer moves, his/her old address is no longer useful so it is simply overwritten),
and archive data ones it’s operational usefulness has ended.
Operational systems are implemented in a relational database, the design may called entity-relationship model,
or ER model. The schema of operational systems are highly accepted to be in third normal form.


\subsection{Analytic Systems}
\label{introduction:analytic-systems}
An analytical system supports the \emph{evaluation} of a business process.
How are orders trending this month versus last?
Where does this put us in comparison to our sales goals for the quarter?
Is a particular marketing promotion having an impact on sales? Who are our best customers?

Interaction with an analytic system takes place through queries that retrieve data
about business processes. Historic data will remain important to the analytic system long after
its operational use has passed.

\textbf{OPERATIONAL SYSTEM VS. ANALYTICAL SYSTEM}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
 & 
Operational System
 & 
Analytic System
\\
\hline
Purpose
 & 
Execution of a business process
 & 
Measurement of a business process
\\
\hline
Primary Interaction Style
 & 
Insert, Update, Delete, Query
 & 
Query
\\
\hline
Scope of Interaction
 & 
Individual transaction
 & 
Aggregated transactions
\\
\hline
Querry Patterns
 & 
Predictable and stable
 & 
Unpredictable and changing
\\
\hline
Temporal Focus
 & 
Current
 & 
Current and historic
\\
\hline
Design Optimaziation
 & 
Update concurrency
 & 
High-performance query
\\
\hline
Design Principle
 & 
Entity-relationship (ER) design in third normal form (3NF)
 & 
Dimensional design
(Starschema or Cube)
\\
\hline
Also Known As
 & 
Transaction System,Online Transaction Processing System (OLTP),Source System
 & 
Data Warehouse System,Data Mart
\\
\hline\end{tabulary}



\subsection{Analytic Databases and Dimensional Design}
\label{introduction:analytic-databases-and-dimensional-design}
The dimensional model of a business process is made up of two components: \emph{measurements} and their \emph{context}.
Known as facts and dimensions, these components are organized into a database design that
facilities a wide variety of analytic usage. Implemented in a relational database, the dimensional
model is called a star schema. Implemented in a multidimensional database, it is known as a cube.
The core of every dimensional model is a set of business metrics that captures how a process is evaluated,
and a description of the context of every measurement \footnote[6]{\sphinxAtStartFootnote%
Excerpt From: Adamson, Christopher. “Star Schema The Complete Reference.” Copyright 2010 by The McGraw-Hill Companies, Inc. All rights reserved. ISBN: 978-0-07-174433-1
}.

\textbf{Purpose}

Analytic systems and operational systems serve fundamentally different purposes.
An operational system supports the execution of a business process, while and
analytic system supports the evaluation of the process \footnote[7]{\sphinxAtStartFootnote%
Excerpt From: Adamson, Christopher. “Star Schema The Complete Reference.” Copyright 2010 by The McGraw-Hill Companies, Inc. All rights reserved. ISBN: 978-0-07-174433-1
}.

\textbf{Measurement and Context}

Dimensional design supports analysis of a business process by modeling how it is
measured. Consider the following business questions:
\begin{itemize}
\item {} 
What are gross margins by product category for June?

\item {} 
What is the average transaction by states level?

\item {} 
What is the return rate by visitors?

\end{itemize}

These questions do not focus on individual activities or transactions.
To answer them, it is necessary to look at a group of transactions - in a bigger picture.
Each of these questions reveals something about how its respective business process is measured.

Every dimensional solution describes a process by capturing what is measured and the context in which the measurements are evaluated \footnote[8]{\sphinxAtStartFootnote%
Excerpt From: Adamson, Christopher. “Star Schema The Complete Reference.” Copyright 2010 by The McGraw-Hill Companies, Inc. All rights reserved. ISBN: 978-0-07-174433-1
}.

\textbf{Facts and Dimensions}

In a dimensional design, measurements are called facts, and context descriptors are called dimensions.
Facts tend to be numeric in value. Elements that are aggregated, summarized, or subtotaled are facts.

\begin{tabulary}{\linewidth}{|L|L|}
\hline

\textbf{FACTS}
 & 
\textbf{DIMENSIONS}
\\
\hline
Amount
 & 
Product
\\
\hline
Min Amount
 & 
Agency
\\
\hline
Max Amount
 & 
Award
\\
\hline & 
Geography
\\
\hline\end{tabulary}



\subsection{The Star Schema}
\label{introduction:the-star-schema}
A dimensional design for a relational database is called a star schema.
Related dimensions are grouped as columns in dimension tables, and the facts
are stored as columns in a fact table.

Dimension tables are not in third normal form. A dimensional model serves a different purpose from ER model.
It is not necessary to isolate repeating values in an environment that doesn't support transaction processing.
When additional normalization is performed within dimensions, in such cases, the schema is referred as a snowflake.
\begin{figure}[h]
\centering
\capstart

\scalebox{0.600000}{\includegraphics{{starschema}.png}}
\caption{Star Schema example}\label{introduction:id16}\end{figure}

\textbf{Dimension Tables}

In a star schema, a dimension table contains columns representing dimensions.
These columns provide context for facts.

\textbf{Fact Table}

At the core of a star schema is the fact table.
Each row in the fact table stores facts at a specific level of detail.
This level of detail is known as the fact table’s grain


\chapter{Building The Data Warehouse}
\label{datawarehouse::doc}\label{datawarehouse:building-the-data-warehouse}

\section{Plan}
\label{datawarehouse:plan}

\subsection{Goals of Data Warehouse}
\label{datawarehouse:goals-of-data-warehouse}
Before we delve into the details of dimensional modeling and implementation, it is helpful
to focus on fundamental goals of the data warehouse. How can we focus on these
these fundamental goals if we are missing the most important thing, the \textbf{data}.

Based on our experience, the data is the universum that drive the bedrock requirements for
the data warehouse. The data comes first, than the technology and the bussiness model.


\subsubsection{Finding the Data}
\label{datawarehouse:finding-the-data}
After couple of weeks of searching we have identified several open to public data sources:
\begin{itemize}
\item {} 
\href{https://www.gov.uk}{UK Gov}

\item {} 
\href{https://openspending.org}{Open Spending}

\item {} 
\href{https://www.data.gov}{U.S. Government’s open data}

\item {} 
\href{https://www.usaspending.gov/Pages/Default.aspx}{Usa Spending Gov}

\end{itemize}

We have decided to looking for government data. We exemined couple of datasets and
we picked \href{https://www.usaspending.gov/Pages/Default.aspx}{Usa Spending Gov}.
Their data looked the most promicing and they provide great
\href{https://www.usaspending.gov/about/PublishingImages/Pages/TheData/USAspending.gov\%20Data\%20Dictionary.pdf}{document}
information about the data fields, description and their formats.

\textbf{Identifying the data source}

All the prime recipient transaction data on \titleref{USAspending.gov \textless{}https://www.usaspending.gov\textgreater{}}
is reported by the federal agencies making contract, grant, loan, and other financial assistance awards.
After identifying the data source and examination of the data sets we have decided to use this data
for our business model and data warehouse model.

\textbf{Mission of data warehouse}

We have concluded the following goals for our data warehouse:

\textbf{The main mission of the data warehouse is to publish the federal organisations data.}
The key success of our data warehouse is whether the data warehouse effectively contributes to the general public.
The success of a data warehouse begins end ends with its users.

The data warehouse is going to be open to public. \textbf{It must make the information easily accessible.}
The content of the data warehouse must be understandable. The data must be intuitive and obvious to the business user,
not merely the developer.

\textbf{The data warehouse must present the federal organisations information consistently.}
Data must be carefully assembled from a variety of sources around the organization, cleansed, quality assured,
and released only when it is fit for user consumption.

\textbf{The data warehouse must be adaptive and resilient to change.}
We want to track changes of federal awards made by federal agencies.
The data warehouse must be designed to handle this change.


\section{Data warehouse Environment}
\label{datawarehouse:data-warehouse-environment}
It is helpful to understand the pieces of the data warehouse before we begin to combine them.
Each component serves specific function.
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{dw_pieces}.jpg}}
\caption{Data warehouse components}\label{datawarehouse:id5}\end{figure}


\subsection{Operational Source systems}
\label{datawarehouse:operational-source-systems}
The  operational system directly captures the execution of a business process,
in our case capture the transactions of federal agencies. The operational source system is outside the data warehouse,
we have no control over the content and format of the data in these systems. In our case the operational system is
\href{https://www.usaspending.gov}{usaspending.gov} where we just download directly from they site a .csv flat file.


\subsection{Data Staging Area}
\label{datawarehouse:data-staging-area}
The data staging is a physical storage area and a set of extract-transformation-load (ETL) jobs.
The data staging are is everything between the source systems and the data presentation area.
This is the stage where we perform various process on the data to fit into data warehouse environment.


\subsection{Data presentation}
\label{datawarehouse:data-presentation}
The data presentation area is where the structured data is organized, stored,
and made available for querying and for analytical applications. The presentation area is based
on online analytic processing (OLAP) technology, this means the data is stored in cubes.


\subsection{Data access tools}
\label{datawarehouse:data-access-tools}
The final piece of the data warehouse is the data access tool. Obviously the first access tool
can be a simple query tool for querying.  We provide to our end users the general public set of
tools for development of reporting, analysis and browsing of data trough our web application using the concept of Cubes.


\section{Data Staging Area}
\label{datawarehouse:id2}
The ETL system is the foundation of the data warehouse. We have design an ETL system that extracts data from the source
system, enforces data quality and consistency, conforms data so that separate flat files can be used together,
and delivers data into presentation layer where we build the application so that end users can make decisions. We haven’t
used any ETL tools every process/job is all hand coded.
\begin{figure}[h]
\centering
\capstart

\scalebox{0.900000}{\includegraphics{{etl}.png}}
\caption{ETL process flow}\label{datawarehouse:id6}\end{figure}


\subsection{Extract}
\label{datawarehouse:extract}
The \href{https://www.usaspending.gov}{usaspending.gov}  website doesn’t provide any API, only single page with the download able link.
The raw data coming from the source system is stored locally on the disk. We have downloaded 4 different .csv flat files.
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{e}.png}}
\caption{Figure {[}5{]}}\label{datawarehouse:id7}\end{figure}

We have examined the .csv flat files. We have encountered various errors when we wanted to load the data into our database.
Here is an example of a source flat file:

\def\SphinxLiteralBlockLabel{\label{datawarehouse:raw-data}}
\SphinxSetupCaptionForVerbatim{literal-block}{Example of raw data}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdq{}07: Direct loan\PYGZdq{},\PYGZdq{}2\PYGZdq{},\PYGZdq{}: Current record\PYGZdq{},\PYGZdq{}\PYGZdq{},\PYGZdq{}4967000\PYGZdq{},\PYGZdq{}UTAH\PYGZdq{},\PYGZdq{}SALT, LAKE”,\PYGZdq{}USA\PYGZdq{},\PYGZdq{}841062671\PYGZdq{},  “”,
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

The empty double quotes PostgreSQL treats as empty string and not as NULL value. When we wanted to insert the data into
the database we have always encountered syntax type error for numeric types. We have decided to get rid of the double
quotes in the flat files. For that reason we have made a little Python script which strip all the double quotes,
in a way that we haven’t disturb data integrity.

\def\SphinxLiteralBlockLabel{\label{datawarehouse:python-script-strip-double-quotes}}
\SphinxSetupCaptionForVerbatim{literal-block}{Script to strip double quotes}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n+nb}{file} \PYG{o+ow}{in} \PYG{n}{csv\PYGZus{}files}\PYG{p}{:}
  \PYG{n}{source} \PYG{o}{=} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n+nb}{file}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{n}{source\PYGZus{}name} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{basename}\PYG{p}{(}\PYG{n+nb}{file}\PYG{p}{)}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{n}{cleaned\PYGZus{}csv} \PYG{o}{=} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{path\PYGZus{}cleaned}\PYG{o}{+}\PYG{n}{source\PYGZus{}name}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}cleaned.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{n}{reader} \PYG{o}{=} \PYG{n}{csv}\PYG{o}{.}\PYG{n}{reader}\PYG{p}{(}\PYG{n}{source}\PYG{p}{)}
  \PYG{n}{writer} \PYG{o}{=} \PYG{n}{csv}\PYG{o}{.}\PYG{n}{writer}\PYG{p}{(}\PYG{n}{cleaned\PYGZus{}csv}\PYG{p}{)}
  \PYG{k}{for} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{reader}\PYG{p}{:}
      \PYG{n}{writer}\PYG{o}{.}\PYG{n}{writerow}\PYG{p}{(}\PYG{n}{row}\PYG{p}{)}
  \PYG{n}{source}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
  \PYG{n}{cleaned\PYGZus{}csv}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

After we have stripped all the double quotes from all  the flat files we have managed to load the data into database.
All the tables start with a prefix \emph{base\_} + name of the flat file from the source. For example for grants: \emph{base\_grants} table.

\textbf{PICTURE}

After we have created the base tables where we have loaded the extracted data from the source.
We started with the highest level of business objectives, identify the likely data from the base tables,
that we believe will support the decisions needed by the business model - which we have documented in the
Dimensional Modelling sections. We started with identifying specific data elements we believed are the most
important for the business model. We analysed the data in the base tables and come to conclusion that data in these
tables must be examined for data quality, completeness, and fitness for the purpose of the data warehouse. For the simplicity
we have designed logical data mapping document. The document contains the data from the source systems throughout to the target
data table, and the exact manipulation of the data required to transform it from its original format to that of its target table.

\textbf{TABLE}


\subsection{Clean}
\label{datawarehouse:clean}
After we have populated our target table, we have created a various jobs to clean the data from the source and consolidate them.
Staging table contains mostly raw text values and numeric only for amounts. Content of the table mostly matches information fromthe source.

Goal of these jobs more specifically:
\begin{itemize}
\item {} 
Cleanse DUNS number, zip code

\item {} 
Cleanse all columns of unwanted special characters (+,\#,@,!)

\item {} 
Replace all foreign entities with a FRGN flag, there are lot of cases when the place of performance of agreements has been outside of United States of America. In most cases these values where mostly incorrect or have been unknown.

\item {} 
Correct state names. There are cases when the state names for United States of America are given only by their abbreviations.

\item {} 
Geography consolidation

\end{itemize}

\textbf{Geography Consolidation}

Table with geography country names sometime contain only their country codes. For those cases a mapping table have been
created where we have specified a mapping of country codes to their valid country names.

The process is depicted in the following image:

\textbf{IMAGE}


\subsection{Load}
\label{datawarehouse:load}
After extracting, cleaning and transformation finally it’s time to populate our dimension tables and fact table.
First we have created the dimensions, we assign every dimension table a surrogate key, we have added a EXCEPT
statement into our INSERT statement for a reason of avoiding duplicates to be inserted into the dimension tables.

This step consisted of the following creates and loads of all structures for analytical processing.
\begin{itemize}
\item {} 
fact table - fact is transcation

\item {} 
dimensions:
* geography
* agency
* recipient
* date
* type of transactions
* award

\end{itemize}

\textbf{Dimensions}

Examples how the dimensions have been created:

\textbf{Geogprahy Dimension}

\def\SphinxLiteralBlockLabel{\label{datawarehouse:create-geography-dimension}}
\SphinxSetupCaptionForVerbatim{literal-block}{Geography Dimension - SQL}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{k}{CREATE} \PYG{k}{TABLE} \PYG{n}{dm\PYGZus{}geography} \PYG{p}{(}
     \PYG{n}{id} \PYG{n+nb}{SERIAL} \PYG{k}{PRIMARY} \PYG{k}{KEY}\PYG{p}{,}
     \PYG{k}{state} \PYG{n+nb}{text}\PYG{p}{,}
     \PYG{n}{city} \PYG{n+nb}{text}\PYG{p}{,}
     \PYG{n}{zip} \PYG{n+nb}{text}\PYG{p}{,}
     \PYG{n}{country} \PYG{n+nb}{text}
 \PYG{p}{)}\PYG{p}{;}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

\textbf{Recipeint Dimension}

\def\SphinxLiteralBlockLabel{\label{datawarehouse:create-recipient-dimension-sql}}
\SphinxSetupCaptionForVerbatim{literal-block}{Recipient Dimension - SQL}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{k}{CREATE} \PYG{k}{TABLE} \PYG{n}{dm\PYGZus{}recipient} \PYG{p}{(}
     \PYG{n}{id} \PYG{n+nb}{SERIAL} \PYG{k}{PRIMARY} \PYG{k}{KEY}\PYG{p}{,}
     \PYG{n}{name} \PYG{n+nb}{text}\PYG{p}{,}
     \PYG{n}{streetaddress} \PYG{n+nb}{text}\PYG{p}{,}
     \PYG{k}{state} \PYG{n+nb}{text}\PYG{p}{,}
     \PYG{n}{city} \PYG{n+nb}{text}\PYG{p}{,}
     \PYG{n}{zip} \PYG{n+nb}{text}\PYG{p}{,}
     \PYG{n}{country} \PYG{n+nb}{text}\PYG{p}{,}
     \PYG{n}{duns} \PYG{n+nb}{text}
 \PYG{p}{)}\PYG{p}{;}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

\textbf{Fact Table}

The initial situation was that we should have built a data warehouse, which is able archive large data by monthly basis.
We have also known that the information is needed for various reports on weekly, monthly, three months periods, and so on.
For this purpose of better query performance we have broke down the fact table into partitions. PostgreSQL supports partitioning
via table inheritance {[}\url{https://www.postgresql.org/docs/9.1/static/ddl-partitioning.html}{]}. We have made the partitioning in a such
a way that every child table inherits from a single master table.

The partitioning have been implemented in the following way:
\begin{enumerate}
\item {} 
We created the master fact table

\end{enumerate}
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
We created the child fact tables, which inherits the master fact table and added checks for dates, because we wanted to
ensure that we have only right data on each partition.Partitions starts from 2015-01-01 and ends 2015-02-01. Each partition contains one month data.

\end{enumerate}

\def\SphinxLiteralBlockLabel{\label{datawarehouse:child-fact-table}}
\SphinxSetupCaptionForVerbatim{literal-block}{Child fact table}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{k}{CREATE} \PYG{k}{TABLE} \PYG{n}{ft\PYGZus{}spending\PYGZus{}2\PYGZus{}15M01}\PYG{p}{(}
     \PYG{k}{PRIMARY} \PYG{k}{KEY} \PYG{p}{(}\PYG{n}{id}\PYG{p}{,} \PYG{n}{last\PYGZus{}modified\PYGZus{}date}\PYG{p}{)}\PYG{p}{,}
     \PYG{k}{CHECK} \PYG{p}{(} \PYG{n}{last\PYGZus{}modified\PYGZus{}date} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n+nb}{DATE} \PYG{l+s+s1}{\PYGZsq{}2015\PYGZhy{}01\PYGZhy{}01\PYGZsq{}} \PYG{k}{and} \PYG{n}{last\PYGZus{}modified\PYGZus{}date} \PYG{o}{\PYGZlt{}} \PYG{n+nb}{DATE} \PYG{l+s+s1}{\PYGZsq{}2015\PYGZhy{}02\PYGZhy{}01\PYGZsq{}}\PYG{p}{)}
 \PYG{p}{)} \PYG{k}{INHERITS} \PYG{p}{(}\PYG{n}{ft\PYGZus{}spending\PYGZus{}2}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
We created indexes to child fact tables to speed up day field usage, because almost all queries are triggered on the date field.

\end{enumerate}

\def\SphinxLiteralBlockLabel{\label{datawarehouse:child-fact-table-indexes}}
\SphinxSetupCaptionForVerbatim{literal-block}{Child table indexes}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{CREATE} \PYG{k}{INDEX} \PYG{n}{idx\PYGZus{}15M01} \PYG{k}{ON} \PYG{n}{ft\PYGZus{}spending\PYGZus{}2\PYGZus{}15M01} \PYG{p}{(}\PYG{n}{last\PYGZus{}modified\PYGZus{}date}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
We wanted our data warehouse to be able to say INSERT INTO \emph{spending} and have the data be redirected into the appropriate partition table.
We have arranged this by creating a suitable trigger function to the master table.

\end{enumerate}

\def\SphinxLiteralBlockLabel{\label{datawarehouse:trigger-function}}
\SphinxSetupCaptionForVerbatim{literal-block}{Trigger Function}
\begin{Verbatim}[commandchars=\\\{\}]
 CREATE OR REPLACE FUNCTION ft\PYGZus{}partition\PYGZus{}trigger()
 RETURNS TRIGGER AS \PYGZdl{}\PYGZdl{}
 BEGIN
   IF
     (NEW.last\PYGZus{}modified\PYGZus{}date \PYGZgt{}= DATE \PYGZsq{}2016\PYGZhy{}01\PYGZhy{}01\PYGZsq{} and NEW.last\PYGZus{}modified\PYGZus{}date \PYGZlt{} DATE \PYGZsq{}2016\PYGZhy{}02\PYGZhy{}01\PYGZsq{}) THEN
     INSERT INTO ft\PYGZus{}spending\PYGZus{}2\PYGZus{}16M01 VALUES (NEW.*);
     ELSIF (NEW.last\PYGZus{}modified\PYGZus{}date \PYGZgt{}= DATE \PYGZsq{}2016\PYGZhy{}02\PYGZhy{}01\PYGZsq{} and NEW.last\PYGZus{}modified\PYGZus{}date \PYGZlt{} DATE \PYGZsq{}2016\PYGZhy{}03\PYGZhy{}01\PYGZsq{}) THEN
     INSERT INTO ft\PYGZus{}spending\PYGZus{}2\PYGZus{}16M02 VALUES (NEW.*);
     ELSIF (NEW.last\PYGZus{}modified\PYGZus{}date \PYGZgt{}= DATE \PYGZsq{}2016\PYGZhy{}03\PYGZhy{}01\PYGZsq{} and NEW.last\PYGZus{}modified\PYGZus{}date \PYGZlt{} DATE \PYGZsq{}2016\PYGZhy{}04\PYGZhy{}01\PYGZsq{}) THEN
     INSERT INTO ft\PYGZus{}spending\PYGZus{}2\PYGZus{}16M03 VALUES (NEW.*);
     ...
     ELSIF (NEW.last\PYGZus{}modified\PYGZus{}date \PYGZgt{}= DATE \PYGZsq{}2015\PYGZhy{}11\PYGZhy{}01\PYGZsq{} and NEW.last\PYGZus{}modified\PYGZus{}date \PYGZlt{} DATE \PYGZsq{}2015\PYGZhy{}12\PYGZhy{}01\PYGZsq{}) THEN
     INSERT INTO ft\PYGZus{}spending\PYGZus{}2\PYGZus{}15M11 VALUES (NEW.*);
     ELSIF (NEW.last\PYGZus{}modified\PYGZus{}date \PYGZgt{}= DATE \PYGZsq{}2015\PYGZhy{}12\PYGZhy{}01\PYGZsq{} and NEW.last\PYGZus{}modified\PYGZus{}date \PYGZlt{} DATE \PYGZsq{}2016\PYGZhy{}01\PYGZhy{}01\PYGZsq{}) THEN
     INSERT INTO ft\PYGZus{}spending\PYGZus{}2\PYGZus{}15M12 VALUES (NEW.*);
     ELSE
       RAISE EXCEPTION \PYGZsq{}DATE OUT OF RANGE!\PYGZsq{};
   END IF;
   RETURN NULL;
 END;
 \PYGZdl{}\PYGZdl{}
 LANGUAGE plpgsql;
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

After creating the trigger function, we created a trigger which calls the trigger function.

\def\SphinxLiteralBlockLabel{\label{datawarehouse:id4}}
\SphinxSetupCaptionForVerbatim{literal-block}{Trigger}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{k}{CREATE} \PYG{k}{TRIGGER} \PYG{n}{insert\PYGZus{}ft\PYGZus{}spending\PYGZus{}2}
   \PYG{k}{BEFORE} \PYG{k}{INSERT} \PYG{k}{ON} \PYG{n}{ft\PYGZus{}spending\PYGZus{}2}
     \PYG{k}{FOR} \PYG{k}{EACH} \PYG{k}{ROW} \PYG{k}{EXECUTE} \PYG{k}{PROCEDURE} \PYG{n}{ft\PYGZus{}partition\PYGZus{}trigger\PYGZus{}2}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

With all of these steps we ensured the our master fact table is available and all UPDATEs. INSERT’s, SELECT’s and DELETE’s goes to the right child tables by date.

Finally it was time to populating the fact table. The fact table has been created simply by transforming cleansed data and joining with prepared dimensions.


\section{Dimensional Modeling}
\label{datawarehouse:dimensional-modeling}

\subsection{Business Process}
\label{datawarehouse:business-process}
The first step in the design is to decide what business process to model by understanding of
the business requirements with an understanding of the available data. In our open government case study,
the general public wants to better understand how the government spends money, what kind of transactions are
made in their neighbourhood. Thus our process is a transactional model. This transactional data will allow us
to analyse what kind of awards are made by federal agencies in which states on what days and to whom.
Brief description of business model that we’ll use in our case study to make dimension and fact tables
more understandable. Imagine a federal agency for example Department of Agriculture making a award for a
recipient 1901 Combine Group, LLC for a combine harvester in Texas on 2015. To summarise it WHICH federal
agency is awarding WHOM for WHAT and WHERE is the place of the performance of the transaction made.


\subsection{Declare the Grain}
\label{datawarehouse:declare-the-grain}
Once the business process has been identified. we faced a serious decision about the granularity of the data warehouse.
What level of data detail should be made available in the dimensional model?
After identifying the data, we had couple of options to choose. We wanted tackling the data at it’s lowest level,
most atomic grain made the most sense. The more detailed and atomic the fact measurement, the more things we know for
sure about federal awards. In this regard, atomic data was the perfect match for the dimensional approach.
Atomic data provides the maximum analytic flexibility because it can be constrained and rolled up in every way possible.
In our case study, the most granular data is an individual transaction made by federal agencies. Because of this level of
grain we ensured maximum dimensionality and flexibility. Providing access to the transactions information gave us very
detailed look at federal award changes. For example, the end users want to see how many transaction were made for one
individual award or how the award has changed over period of time, if the agency made a modification to an award,
reduced a portion of the original award amount or made additional funding.None of them could have been answered if we wouldn't
elected the lowest granularity just the summarised data.


\subsection{Choose the Dimensions}
\label{datawarehouse:choose-the-dimensions}
After we have declared the grain of the fact table, the recipient, agency, date, geography, award, dimensions fall out immediately.
We assume that the calendar date is the date when the award was signed.

In our case study we have decided on the following dimensions:
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{dw_schema_tbd}.png}}
\caption{Preliminary star schema.}\label{datawarehouse:id8}\end{figure}

Dimension tables are not in third normal form. A dimensional model serves a different purpose from ER model.
It wasn’t necessary to isolate repeating values in an environment that doesn't support transaction processing.
If we would have made additional normalisation within dimensions, we would end up with the schema that is referred as a snowflake.
We have encouraged to resist the urge to snowflake given our to primary design, ease of use and performance.
\begin{itemize}
\item {} 
Snowflaked tables makes for much more complex presentation.

\item {} 
Database design will struggle with the complexity of the snowflaked schema.

\item {} 
Numerous tables and joins usually translate into slower query performance.

\item {} 
Minor disk space savings.

\item {} 
Snowflaking slows down the user’s ability to browse within the dimension.

\end{itemize}

Dimension tables also contain key columns that uniquely identify something in an operational system.
These key columns are referred to as natural keys. The separation of surrogate keys and natural keys
allows the data warehouse to track changes, even if the originating operational system does not.


\subsection{Identify Facts}
\label{datawarehouse:identify-facts}
At the core of a star schema is the fact table. In addition to presenting the facts, the fact table includes surrogate
keys that refer to each of the associated dimension tables. Each row in the fact table stores facts at a specific
level of detail of our grain that we have declared. Facts tend to be numeric in value. We have made the decision
that the award amount is going to be our fact measurement which will appear in our fact table. We have decided
to stored physically in the data warehouse only one fact the award mount, which is additive across all dimensions.


\subsection{Suroggate Keys}
\label{datawarehouse:suroggate-keys}
In the star schema, each dimension table is given a surrogate key. This column is a unique identifier,
created exclusively for the data warehouse. The surrogate key is the primary key of the dimension table.
In our case, surrogate keys are randomly generated integers that are assigned sequentially when populate
dimension tables during the ETL process. For example, the first recipient record is assigned a recipient
surrogate key with the value of 1, the next recipient record is assigned recipient key with value 2, and
so forth. The surrogate keys serve to join the dimension tables to the fact table. One of the most important
reasons why are we using surrogate keys and doesn’t just rely on natural keys from the source system is to
support handling changes to dimension table attributes.


\section{Dimensional Table Attributes}
\label{datawarehouse:dimensional-table-attributes}

\subsection{Date Dimension}
\label{datawarehouse:date-dimension}
We have started with the date dimension, which is nearly guaranteed to be in every data warehouse.
Unlike other dimensions, we could have build the date dimension in advance. We have put 10 - 20 years of rows representing
days in the table so that we can cover the history we have stored, as well as several years in the future.
From our data we know that first public awards come from the year 2008, so even 10 years worth of days is only about 3650 rows,
which is relatively small dimension table.
\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{date_dim}.png}}
\caption{Date Dimension}\label{datawarehouse:id9}\end{figure}

Each column in the date dimension table is defined by the particular day that the row represents.
The full day columns represents the full date when the award have been signed in format YYYY-MM-DD it data type is date.
The year columns represents the fiscal year of the award and it is a singe integer type. This column is useful for slicing,
when we want filter a particular year, for example we want to show all the transactions made in the year of 2015.
The month column represents the month number in the year (1, 2, …, 12). The month name columns represents the name of the month
(January, February etc.). Similarly, as month we have a day column, which represents the day number in calendar month columns
starts with 1 at the beginning of each month an it is depending on the particular month. The day in the year columns is a single
integer which is representing the day number in the year, it starts from 1 and runs to 365, depending on what year is.
The weekday name column represents the name of the day when the award have been signed, such as Wednesday. It has string data type.
We have also included calendar week column. We have included quarter number (Q1,…, Q4). Columns like year, quarter, month, calendar week,
day all these integers support as simple date filters. Figure 2.5 illustrates several rows from a partial date dimension table.
\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{date_dim_detail}.png}}
\caption{Sample of Date Dimension}\label{datawarehouse:id10}\end{figure}


\subsection{Product Dimension}
\label{datawarehouse:product-dimension}\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{product_dim}.png}}
\caption{Product Dimension}\label{datawarehouse:id11}\end{figure}

The product dimension describes every product or service for a particular award.
While a typical reasonable product dimension table would have 50 or more descriptive attributes, in our data warehouse
we have only one product name attribute column. It is sourced from the operational system as a single attribute column with no hierarchy.
The only function of the product dimension is to give a descriptive attribute of each award. In our case there is only
2492 different values in the product dimension table. Viewed in this manner, we can only drill down on one level of the product
dimension which provides us information about the award amount and quantity by product name. For example here is simple
report overview, we have summarised the award amount and quantity (count) by product name.
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{product_sample}.png}}
\caption{Drill down sample of Product Dimension}\label{datawarehouse:id12}\end{figure}


\subsection{Geography Dimension}
\label{datawarehouse:geography-dimension}\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{geography_dim}.png}}
\caption{Geography Dimension}\label{datawarehouse:id13}\end{figure}

The geography dimension describes every transactions places of performance for recipients. It is primary a geographic
dimension in our case study. Each row can be thought of as a location where an award has been made. Because of this we,
can drill down / roll up any geography dimension attribute, such as country, state, city, zip code. These columns
attributes are representing in the geography dimension a simple hierarchy for a single row.


\subsection{Recipient Dimension}
\label{datawarehouse:recipient-dimension}\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{recipient_dim}.png}}
\caption{Recipient Dimension}\label{datawarehouse:id14}\end{figure}

The recipient dimension describes recipient transactions. The dimension contains row for each recipient,
along with descriptive attributes such as street address, state, city etc. We capture only the recipient name,
address information and DUNS number. It is used to establish a business credit file, which is often referenced by
lenders and potential business partners to help predict the reliability and/or financial stability of the company
in question {[}\url{http://www.dnb.com/duns-number.html}{]}. Recipient geographic attributes have been complicated to
dealing with. We wanted to avoid snowflaking  as we have mentioned in Choose Dimension chapter. In our model
recipients typically have multiple addresses, but every transaction is geocoded on the prime recipient address.
This means we have unique row for each recipient based on his geographic attributes. The street address column
is our natural key to identify each unique recipient.

A sample set of name and location attributes for individual recipient:

\begin{tabulary}{\linewidth}{|L|L|L|L|L|L|L|L|}
\hline

1
 & 
Abbewood
 & 
1002 west 23rd st.
 & 
Alabama
 & 
Panama City
 & 
32405
 & 
United States of America
 & 
627189244
\\
\hline\end{tabulary}



\subsection{Transaction Type Dimension}
\label{datawarehouse:transaction-type-dimension}\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{transactiontype_dim}.png}}
\caption{Transaction Type Dimension}\label{datawarehouse:id15}\end{figure}

The transaction type dimension describes the type of award. It is our smallest dimension consists only of 4 unique row.
The transaction status columns describe the status of the transaction active/inactive. The category columns is our key
attribute in the dimension which describe what kind of transaction is reported by the federal agencies making contract,
grant, loan, and other financial assistance award.
\begin{itemize}
\item {} 
Contract is an agreement between the federal government and a prime recipient to execute goods and services for a fee.

\item {} 
Grant is type of federal award that requires an application process and include payments to non-federal entities for
a defined purpose in which services are not rendered to the federal government.

\item {} 
Loan is type of federal awards that the borrower will eventually pay back to the government.

\item {} 
Other Financial Assistance includes direct payments to individuals, insurance payments, and other types of assistance payments.

\end{itemize}

The transaction description columns provides us descriptive information of types of awards.


\subsection{Award Dimension}
\label{datawarehouse:award-dimension}\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{award_dim}.png}}
\caption{Award Dimension}\label{datawarehouse:id16}\end{figure}

The award dimension is potentially the most important dimension in our schema. The grain of the fact table has been stated
as “awards at the award modification level of detail”. This has been achieved by adding award identifiers from the source
system to identify individual awards: the award\_id and award\_mod. Together with this two attributes we have achieved to
uniquely identify each fact table rows. Because of this approach, each fact table row represents exactly one award
or award modification, the award dimension and the fact table contain the same number of rows.

\textbf{Slowly Changing Dimension}

As our grain dictates, we need to track changes over time. The award modification number is are natural key identifier
carried over from the source system. The award modification number uniquely identify a corresponding entity in our source system.
For example, an individual award is identified by award id in our source system, which uniquely identify a contract or agreement.
With the combination of the natural key award mod from the source system we have achieved to identify each award modification.
The use of surrogate key allows the data warehouse uniquely identify each transaction and respond to changes in the source system.
This allows us to track history changes of awards. With these dimension attributes we have specified a strategy to handle change.
In other words, when an attribute value change in the operational system, we will respond to this change in our dimensional model.
We refer this kind of change as slowly changing dimension. We made the claim earlier about our goals of the data warehouse was to
represent awards correctly. Thats why we have decided to use a technique called SCD Type 2. Using the type 2 approach, when an
agency make a modification to an award (for example restrict the original award or make additional funding) we create a new award
dimension row for the given award to reflect new award amount.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

1
 & 
1PIC355199
 & 
c5e18f14-82cf-4e64-b1b0-cd767735e330
\\
\hline
2
 & 
1PIC355199
 & 
c8130052-2d0b-41f2-9b2b-84f0f3da9d18
\\
\hline
3
 & 
1PIC355199
 & 
c8c24821-38fe-463f-9fe5-2f3f6db054fc
\\
\hline
4
 & 
1PIC355199
 & 
cc809f09-595b-455a-af92-3498a3c97a90
\\
\hline
5
 & 
Geography
 & 
cd4f8125-a714-4565-8d72-4700f522d54a
\\
\hline\end{tabulary}


Now we can see why the the award dimension key can’t be the Award ID natural key. We need different surrogate keys
for the same award id. Each surrogate keys identifies a unique transaction that was true for a span of time.
This method accurately track slowly changing dimensions attributes.


\subsection{Federal Award Transactions}
\label{datawarehouse:federal-award-transactions}
As we have moved from the dimensional design, this is how our federal award transaction business model looks like for now.
\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{starschema_final}.png}}
\caption{Star Schema}\label{datawarehouse:id17}\end{figure}


\chapter{OLAP}
\label{olap::doc}\label{olap:olap}
The presentation area is based on online analytic processing (OLAP) technology, the data is stored in cubes.


\section{Cubes - OLAP Framework}
\label{olap:cubes-olap-framework}
Cubes is a light-weight Python framework and set of tools for development of reporting and analytical applications,
Online Analytical Processing (OLAP), multidimensional analysis and browsing of aggregated data \phantomsection\label{olap:id1}{\hyperref[olap:cubes]{\crossref{{[}Cubes{]}}}}.

The framework models the data as a cube with multiple dimensions:
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{cube-dims_and_cell}.png}}
\caption{Cubes Dimension and Cell}\label{olap:id11}\end{figure}

Core cube features that we have implemented:
\begin{itemize}
\item {} 
\textbf{Workspace} - Cubes analytical workspace

\item {} 
\textbf{Model} - Description of data (metadata): cubes, dimensions, hierarchies, attributes, labels etc.

\item {} 
\textbf{Browser} - Aggregation browsing, slicing-and-dicing, drill-down.

\item {} 
\textbf{Backend} - built-in ROLAP backend which uses Postgres SQL database using SQLAlchemy.

\item {} 
\textbf{Server} - WSGI HTTP server for Cubes

\end{itemize}


\section{Analytical Workspace}
\label{olap:analytical-workspace}
Everything in Cubes happens in an analytical workspace. It contains cubes, maintains connections to the data stores
(with cube data), provides connection to external cubes and more \phantomsection\label{olap:id2}{\hyperref[olap:workspace]{\crossref{{[}workspace{]}}}}.
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{cubes-workspace_simplified-2}.png}}
\caption{Cubes Workspace}\label{olap:id12}\end{figure}

The workspace properties are specified in a configuration file \emph{slicer.ini}.
First we have specified the workspace - cubes workspace configuration.
We have provided the workspace our \emph{model.json} metadata dictionary. We have added a logging
path where to log file for better understanding of debug logs.

\def\SphinxLiteralBlockLabel{\label{olap:workspace-configuration}}
\SphinxSetupCaptionForVerbatim{literal-block}{slicer.ini}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{workspace}\PYG{p}{]}
\PYG{n}{log}\PYG{p}{:} \PYG{n}{error}\PYG{o}{.}\PYG{n}{log}
\PYG{n}{log\PYGZus{}lever}\PYG{p}{:} \PYG{n}{debug}
\PYG{n}{model}\PYG{p}{:} \PYG{n}{model}\PYG{o}{.}\PYG{n}{json}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

For the server section we have defined the HTTP WSGI OLAP Server.
The host where the server runs. Port on which the the server listens.
We have allowed Cross-origin resource sharing header for the CubesViewer.

\def\SphinxLiteralBlockLabel{\label{olap:server-configuration}}
\SphinxSetupCaptionForVerbatim{literal-block}{slicer.ini}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{server}\PYG{p}{]}
\PYG{n}{host}\PYG{p}{:} \PYG{n}{localhost}
\PYG{n}{port}\PYG{p}{:} \PYG{l+m+mi}{5000}
\PYG{n}{prettyprint}\PYG{p}{:} \PYG{n}{true}
\PYG{n+nb}{reload}\PYG{p}{:} \PYG{n}{true}
\PYG{n}{allow\PYGZus{}cors\PYGZus{}origin}\PYG{p}{:} \PYG{n}{http}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{localhost}\PYG{p}{:}\PYG{l+m+mi}{8000}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

We have specified our data store - the database containing the cube’s data.
We have defined the prefix for dimension \emph{dm\_} and for fact \emph{ft\_} this allows the
mapper to find dimension and fact tables in the database.

\def\SphinxLiteralBlockLabel{\label{olap:store-configuration}}
\SphinxSetupCaptionForVerbatim{literal-block}{slicer.ini}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{store}\PYG{p}{]}
\PYG{n+nb}{type}\PYG{p}{:} \PYG{n}{sql}
\PYG{n}{url}\PYG{p}{:} \PYG{n}{postgresql}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{richardbanyi}\PYG{p}{:}\PYG{n}{dpcu923}\PYG{n+nd}{@localhost}\PYG{p}{:}\PYG{l+m+mi}{5432}\PYG{o}{/}\PYG{n}{usaspending}
\PYG{n}{fact\PYGZus{}prefix}\PYG{p}{:} \PYG{n}{ft\PYGZus{}}
\PYG{n}{dimension\PYGZus{}prefix}\PYG{p}{:} \PYG{n}{dm\PYGZus{}}
\PYG{n}{debug}\PYG{p}{:} \PYG{n}{true}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

The model section contains list of models. We have defined only one model.

\def\SphinxLiteralBlockLabel{\label{olap:list-of-models}}
\SphinxSetupCaptionForVerbatim{literal-block}{slicer.ini}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{models}\PYG{p}{]}
\PYG{n}{main}\PYG{p}{:} \PYG{n}{model}\PYG{o}{.}\PYG{n}{json}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\section{Logical Model and Metadata}
\label{olap:logical-model-and-metadata}
Logical model describes the data from user’s or analyst’s perspective: data how they are being measured,
aggregated and reported. Model is independent of physical implementation of data. This physical
independence makes it easier to focus on data instead on ways of how to get the data in understandable form \phantomsection\label{olap:id3}{\hyperref[olap:model]{\crossref{{[}model{]}}}}.

Analyst or report writers, anyone who will access the web application do not have to know where name
of an government organisation or recipient name is stored, nor do they have to know where is data stored
they only ask for \emph{agency.name} or \emph{recipient.name}.

Example: User wants to find out the award amounts by geography which has four levels:country level, state level,
city level and zip level. The original data is stored in the physical database in the geography table. User doesn’t
have to know where the data are stored, he just queries for the geography.state and geography.city and will get the proper data.
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{logical-to-physical}.png}}
\caption{Logical to Physical Mapping}\label{olap:id13}\end{figure}


\section{Model}
\label{olap:id4}
The logical model is described using model metadata dictionary. The content is description of logical objects,
physical storage and other additional information \footnote[9]{\sphinxAtStartFootnote%
Author: Stefan Urbanek,
Date: 2010-2014,
Online: Model metadata
Availailable: \href{http://cubes.readthedocs.io/en/v1.0.1/model.html}{Cubes  Model Metadata}
}.
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{cubes-model_metadata}.png}}
\caption{Cubes - Model Metadata}\label{olap:id14}\end{figure}

\def\SphinxLiteralBlockLabel{\label{olap:model-metadata}}
\SphinxSetupCaptionForVerbatim{literal-block}{Model Example}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}usaspending\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Federal Award Transactions\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}description\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Federal Award Transactions of United States of America\PYGZdq{}}
    \PYG{l+s+s2}{\PYGZdq{}cubes\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}\PYG{p}{...}\PYG{p}{]}
    \PYG{l+s+s2}{\PYGZdq{}dimensions\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}\PYG{p}{...}\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

In our case the logical part of the model description consists of the following attributes:
\textbf{Name} is the name of our model, \textbf{label} and \textbf{descriptions} is used for human readable label/descriptions
it’s optional. \textbf{Cubes} is a list of cubes metadata, we have defined only one cube for our case study spending.
\textbf{Dimensions} is a list of dimension metadata.

The physical part of the model description consists of the following attributes:

\textbf{Joins} we have only specified backend join specification, it is used to match joins in the cube.


\subsection{Cubes}
\label{olap:id7}
Cube descriptions are stored as a dictionary for key \emph{cubes}.

\def\SphinxLiteralBlockLabel{\label{olap:cubes-metadata}}
\SphinxSetupCaptionForVerbatim{literal-block}{Cubes Example}
\begin{Verbatim}[commandchars=\\\{\}]
  \PYGZob{}
      \PYGZdq{}name\PYGZdq{}: \PYGZdq{}usaspending\PYGZdq{},
      \PYGZdq{}label\PYGZdq{}: “Federal Award Transactions\PYGZdq{},
      \PYGZdq{}dimensions\PYGZdq{}: [ \PYGZdq{}date\PYGZdq{}, ... ],
      \PYGZdq{}measures\PYGZdq{}: [...],
      \PYGZdq{}aggregates\PYGZdq{}: [...],
      \PYGZdq{}details\PYGZdq{}: [...],
  \PYGZcb{}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\subsection{Measures and Aggregates}
\label{olap:measures-and-aggregates}\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{cubes-measure_vs_aggregate}.png}}
\caption{Cubes - Measures vs. Aggreates}\label{olap:id15}\end{figure}

The measure is numerical property of a fact. It represent the award\_amount column in the physical model.

\def\SphinxLiteralBlockLabel{\label{olap:measures}}
\SphinxSetupCaptionForVerbatim{literal-block}{Measures}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{l+s+s2}{\PYGZdq{}measures\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}
   \PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}amount\PYGZdq{}} \PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Award amount\PYGZdq{}}\PYG{p}{\PYGZcb{}}
 \PYG{p}{]}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

Aggregates is a list of aggregates that we have provided for the measure.

\def\SphinxLiteralBlockLabel{\label{olap:list-of-aggregates}}
\SphinxSetupCaptionForVerbatim{literal-block}{Aggregates}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{l+s+s2}{\PYGZdq{}aggregates\PYGZdq{}}\PYG{o}{:}  \PYG{p}{[}
   \PYG{p}{\PYGZob{}}
       \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}sum\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Total Award Amount\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}function\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}sum\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}measure\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}amount\PYGZdq{}}
   \PYG{p}{\PYGZcb{}}\PYG{p}{,}
   \PYG{p}{\PYGZob{}}
       \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}transaction\PYGZus{}count\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Total Transactions\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}function\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}count\PYGZdq{}}
   \PYG{p}{\PYGZcb{}}\PYG{p}{,}
   \PYG{p}{\PYGZob{}}
       \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}min\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Min Amount\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}measure\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}amount\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}function\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}min\PYGZdq{}}
   \PYG{p}{\PYGZcb{}}\PYG{p}{,}
   \PYG{p}{\PYGZob{}}
       \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}max\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Max Amount\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}measure\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}amount\PYGZdq{}}\PYG{p}{,}
       \PYG{l+s+s2}{\PYGZdq{}function\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}max\PYGZdq{}}
   \PYG{p}{\PYGZcb{}}
 \PYG{p}{]}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

Note that item\_count aggregate - it counts number of the facts within the cell. No measure required
as a source for the aggregate. It helps to count the number of the transactions.


\subsection{Mappings}
\label{olap:mappings}
The most important part of the OLAP on top of the start schema is mapping of the logical
attributes to their physical attributes. In SQL database the physical attributes are stored in columns,
which belongs to tables, which are part of the starcshema.
\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{mapping_logical_to_physical}.png}}
\caption{Cubes - Mapping Logical to Physical}\label{olap:id16}\end{figure}

Example:

For data browsing, we have defined mappings, so that Cubes framework has know where logical attributes are physically stored.
With this the Cubes framework know which tables are related to the cube spending and how they are joined together.

The are two ways how the mapping is being done:
* Imlicit
* Explicit

We have chosen the implicit declarations which has been most straightforward and simple.
With implicit mapping we have matched a database schema with logical model and have avoided additional specific mapping metadata.

This is how it would looked like if we would have chosen explicit mappings:

\def\SphinxLiteralBlockLabel{\label{olap:explicit-mapping}}
\SphinxSetupCaptionForVerbatim{literal-block}{Explicit Mapping}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{l+s+s2}{\PYGZdq{}mappings\PYGZdq{}}\PYG{o}{:} \PYG{p}{\PYGZob{}}
   \PYG{l+s+s2}{\PYGZdq{}product.name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}dm\PYGZus{}products.product\PYGZus{}name\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

The mapping process look like this:
\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{mapping-overview}.png}}
\caption{Cubes - Mapping Overview}\label{olap:id17}\end{figure}


\subsection{Facts}
\label{olap:facts}
Cubes looks for fact table with the same name as cube name. We have specified prefix for every fact table with
\emph{ft\_} in the workspace configuration \emph{slicer.ini}.
\begin{itemize}
\item {} 
Cube is named \emph{spending}: the framework looks for a fact table named \emph{spending}.

\item {} 
\textbf{fact table name = fact table prefix + fact table name}

\end{itemize}


\subsection{Dimensions}
\label{olap:dimensions}
Same as for fact tables cubes looks for dimension table with the same name as dimension name.
We have specified prefix for every dimension table with \emph{dm\_} in the workspace configuration \emph{slicer.ini}.
We have design our dimension columns to have the same name as dimension attributes.
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{dimension_attribute_prefix_map}.png}}
\caption{Cubes - Dimension attribute prefix}\label{olap:id18}\end{figure}
\begin{itemize}
\item {} 
\textbf{dimension table name = dimension prefix + dimension name}

\end{itemize}

An example of product dimension:

\def\SphinxLiteralBlockLabel{\label{olap:product-dimension}}
\SphinxSetupCaptionForVerbatim{literal-block}{Product dimension}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{p}{\PYGZob{}}
   \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}product\PYGZdq{}}\PYG{p}{,}
   \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Product\PYGZdq{}}\PYG{p}{,}
   \PYG{l+s+s2}{\PYGZdq{}attributes\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}
       \PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}id\PYGZdq{}} \PYG{p}{\PYGZcb{}}\PYG{p}{,}
       \PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}product\PYGZus{}name\PYGZdq{}} \PYG{p}{\PYGZcb{}}
   \PYG{p}{]}\PYG{p}{,}
   \PYG{l+s+s2}{\PYGZdq{}levels\PYGZdq{}}\PYG{o}{:} \PYG{p}{[} \PYG{p}{...} \PYG{p}{]}\PYG{p}{,}
   \PYG{l+s+s2}{\PYGZdq{}hierarchies\PYGZdq{}}\PYG{o}{:} \PYG{p}{[} \PYG{p}{...} \PYG{p}{]}
 \PYG{p}{\PYGZcb{}}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\subsection{Joins}
\label{olap:joins}
Tables are joined by matching single-column - surrogate keys.
Joins are defined as an ordered list. We had to define the column reference for both master
table and a table with details. The order of joins has to be from master to detail.

\def\SphinxLiteralBlockLabel{\label{olap:join}}
\SphinxSetupCaptionForVerbatim{literal-block}{Join example}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{l+s+s2}{\PYGZdq{}joins\PYGZdq{}}\PYG{o}{:}  \PYG{p}{[}
     \PYG{p}{\PYGZob{}}
         \PYG{l+s+s2}{\PYGZdq{}master\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}date\PYGZus{}id\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}detail\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}dm\PYGZus{}date.id\PYGZdq{}}
     \PYG{p}{\PYGZcb{}}\PYG{p}{,}
 \PYG{p}{]}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

Cubes supports three join methods \emph{match, detail and master}.

\emph{match} (default) – the keys from both master and detail tables have to match – INNER JOIN
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{cubes-sql_joins-match}.png}}
\caption{Cubes - Join Match}\label{olap:id19}\end{figure}


\subsection{Hierarchies and Levels}
\label{olap:hierarchies-and-levels}
Dimensions can have more than one level. Date dimension has 8 levels. Which every of these levels has it’s own attributes.
The month level is represented by two attributes month an integer data type which describes the mont of the year and
second attribute month name.

\def\SphinxLiteralBlockLabel{\label{olap:month-level}}
\SphinxSetupCaptionForVerbatim{literal-block}{Levels - Month}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{l+s+s2}{\PYGZdq{}levels\PYGZdq{}}\PYG{o}{:}  \PYG{p}{[}
     \PYG{p}{\PYGZob{}}
         \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Month\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}key\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}label\PYGZus{}attribute\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZus{}name\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}attributes\PYGZdq{}}\PYG{o}{:}  \PYG{p}{[}
             \PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZdq{}} \PYG{p}{\PYGZcb{}}\PYG{p}{,}
             \PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZus{}name\PYGZdq{}} \PYG{p}{\PYGZcb{}}
         \PYG{p}{]}
     \PYG{p}{\PYGZcb{}}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

For date dimension we have created multiple ways of organising attributes into hierarchies.
The date can be composed of \emph{year-month-day} or \emph{year-quarter-month-day}.
We have defined four different hierarchies.

First we have defined all possible level.s Then created list of hierarchies where we specified
order of the levels for the particularly hierarchy.

\def\SphinxLiteralBlockLabel{\label{olap:ymd}}
\SphinxSetupCaptionForVerbatim{literal-block}{Hierarchies - Year-Month-day}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{l+s+s2}{\PYGZdq{}hierarchies\PYGZdq{}}\PYG{o}{:}  \PYG{p}{[}
     \PYG{p}{\PYGZob{}}
         \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}ymd\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Y\PYGZhy{}M\PYGZhy{}D\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}levels\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}year\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}month\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}day\PYGZdq{}}\PYG{p}{]}
     \PYG{p}{\PYGZcb{}}
 \PYG{p}{]}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\subsection{User-oriented Metadata}
\label{olap:user-oriented-metadata}
For a better understanding to users we have added labels for parts of the model that are being displayed to the users.
They are used for report tables as column headings or as a filter of description. We have specified them for
every object model (cube, dimension, level, attribute etc.) with the label attribute. The \emph{key} attribute is used
for filtering and \emph{label\_attribute} is used for the data to be displayed in the user interface (labels).

\def\SphinxLiteralBlockLabel{\label{olap:labels}}
\SphinxSetupCaptionForVerbatim{literal-block}{Labels}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{l+s+s2}{\PYGZdq{}levels\PYGZdq{}}\PYG{o}{:}  \PYG{p}{[}
     \PYG{p}{\PYGZob{}}
         \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}Month\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}key\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}label\PYGZus{}attribute\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZus{}name\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}attributes\PYGZdq{}}\PYG{o}{:}  \PYG{p}{[}
             \PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZdq{}} \PYG{p}{\PYGZcb{}}\PYG{p}{,}
             \PYG{p}{\PYGZob{}} \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}month\PYGZus{}name\PYGZdq{}} \PYG{p}{\PYGZcb{}}
         \PYG{p}{]}
     \PYG{p}{\PYGZcb{}}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{schema-label_attributes}.png}}
\caption{Cubes - Labels}\label{olap:id20}\end{figure}


\section{Slicer Server}
\label{olap:slicer-server}
Cubes framework provides a WSGI server which covers most of the Cube logical
model metadata and aggregation browsing functionality. We have already configured the server at beginning of the OLAP section.

To get a version of the server we just simple request a GET request.

Request \titleref{GET /version}.

\def\SphinxLiteralBlockLabel{\label{olap:slicer-server-get-version}}
\SphinxSetupCaptionForVerbatim{literal-block}{Slicer Server - GET /version}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}version\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}1.1\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}api\PYGZus{}version\PYGZdq{}}\PYG{o}{:} \PYG{l+m+mi}{2}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}server\PYGZus{}version\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}1.1\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

To get a list of information about served cubes.

Request \titleref{GET /cubes}.

\def\SphinxLiteralBlockLabel{\label{olap:slicer-server-get-cubes}}
\SphinxSetupCaptionForVerbatim{literal-block}{Slicer Server - GET /cubes}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{p}{[}
    \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}label\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}spending\PYGZdq{}}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}category\PYGZdq{}}\PYG{o}{:} \PYG{k+kc}{null}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}name\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}spending\PYGZdq{}}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}info\PYGZdq{}}\PYG{o}{:} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}
\PYG{p}{]}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

\textbf{Aggregation and Browsing}

The core data and analytical functionality is accessed thought the following requests:
\begin{itemize}
\item {} 
\titleref{/cube/\textless{}name\textgreater{}/aggregate} - aggregate measures, summary, generate drill-down, slice\&dice.

\item {} 
\titleref{/cube/\textless{}name\textgreater{}/members/\textless{}dim\textgreater{}} - list of dimension members.

\item {} 
\titleref{/cube/\textless{}name\textgreater{}/facts} - list facts within cell.

\item {} 
\titleref{/cube/\textless{}name\textgreater{}/fact} - return a single fact.

\item {} 
\titleref{/cube/\textless{}name\textgreater{}/cell} - describe cell.

\end{itemize}

The cells - part of the cube we are interested in are specified by cut. The cut in the URL are given as a parameter cut.

For example:
\begin{itemize}
\item {} 
\titleref{cut=date:2015}

\item {} 
\titleref{cut=geography:usa,california}

\end{itemize}

\textbf{Aggregate}

The server always returns the aggregation results as JSON. The result contain keys:
summary - represents the aggregation of the whole cell specified in the cut
aggregates - list of aggregate names that are considered in the aggregation
cell - list of dictionaries describing the cell cuts.

\def\SphinxLiteralBlockLabel{\label{olap:slicer-server-aggregate}}
\SphinxSetupCaptionForVerbatim{literal-block}{Slicer Server - Aggregate}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{p}{\PYGZob{}}
     \PYG{l+s+s2}{\PYGZdq{}summary\PYGZdq{}}\PYG{o}{:} \PYG{p}{\PYGZob{}}
         \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}sum\PYGZdq{}}\PYG{o}{:} \PYG{l+m+mf}{275669527432.41}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}max\PYGZdq{}}\PYG{o}{:} \PYG{l+m+mf}{9999999999.0}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}min\PYGZdq{}}\PYG{o}{:} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1604823842.0}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}transaction\PYGZus{}count\PYGZdq{}}\PYG{o}{:} \PYG{l+m+mi}{1550767}
     \PYG{p}{\PYGZcb{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}remainder\PYGZdq{}}\PYG{o}{:} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}cells\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}aggregates\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}
         \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}sum\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}transaction\PYGZus{}count\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}min\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}award\PYGZus{}max\PYGZdq{}}
     \PYG{p}{]}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}cell\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}
         \PYG{p}{\PYGZob{}}
             \PYG{l+s+s2}{\PYGZdq{}type\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}point\PYGZdq{}}\PYG{p}{,}
             \PYG{l+s+s2}{\PYGZdq{}dimension\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}date\PYGZdq{}}\PYG{p}{,}
             \PYG{l+s+s2}{\PYGZdq{}hierarchy\PYGZdq{}}\PYG{o}{:} \PYG{l+s+s2}{\PYGZdq{}ymd\PYGZdq{}}\PYG{p}{,}
             \PYG{l+s+s2}{\PYGZdq{}level\PYGZus{}depth\PYGZdq{}}\PYG{o}{:} \PYG{l+m+mi}{1}\PYG{p}{,}
             \PYG{l+s+s2}{\PYGZdq{}invert\PYGZdq{}}\PYG{o}{:} \PYG{k+kc}{false}\PYG{p}{,}
             \PYG{l+s+s2}{\PYGZdq{}hidden\PYGZdq{}}\PYG{o}{:} \PYG{k+kc}{false}\PYG{p}{,}
             \PYG{l+s+s2}{\PYGZdq{}path\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}
                 \PYG{l+s+s2}{\PYGZdq{}2015\PYGZdq{}}
             \PYG{p}{]}
         \PYG{p}{\PYGZcb{}}
     \PYG{p}{]}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}attributes\PYGZdq{}}\PYG{o}{:} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}has\PYGZus{}split\PYGZdq{}}\PYG{o}{:} \PYG{k+kc}{false}
 \PYG{p}{\PYGZcb{}}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\section{Slicing and Dicing}
\label{olap:slicing-and-dicing}

\subsection{Browser}
\label{olap:browser}
The aggregation, slicing, dicing, browsing of the multi-dimensional data is being done by an AggregationBrowser.

\def\SphinxLiteralBlockLabel{\label{olap:workspace-browser}}
\SphinxSetupCaptionForVerbatim{literal-block}{Workspace \& Browser initilization}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{cubes} \PYG{k+kn}{import} \PYG{n}{Workspace}

\PYG{n}{workspace} \PYG{o}{=} \PYG{n}{Workspace}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{slicer.ini}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{browser} \PYG{o}{=} \PYG{n}{workspace}\PYG{o}{.}\PYG{n}{browser}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

Here we initialised our workspace, and create and initialises the aggregation browser.
\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{browser-package}.png}}
\caption{Cubes - Browser aggregate}\label{olap:id21}\end{figure}


\subsection{Cells and Cuts}
\label{olap:cells-and-cuts}
Cell defines a point of interest – portion of the cube to be aggergated or browsed.
\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{cubes-slice_and_dice-cell}.png}}
\caption{Cubes - Slice and Dice cell}\label{olap:id22}\end{figure}

There are three types of cells: point – defines a single point in a dimension at a
particular level; range – defines all points of an ordered dimension (such as date)
within the range and set – collection of points \phantomsection\label{olap:id8}{\hyperref[olap:cells]{\crossref{{[}cells{]}}}}.
\begin{figure}[h]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{cubes-point-range-set-cut}.png}}
\caption{Cubes - Cuts}\label{olap:id23}\end{figure}

Points are defined as dimension paths – list of dimension level keys. For example a date path for
3rd of June 2015 would be: {[}2015, 6, 3{]}. For the month June it would be {[}2015, 6{]}
and for the whole year of 2015 it would be {[}2015{]}.
In python the cuts for “transactions from June of 2010 to June of 2012 for geography United States of America” are defined as:

\def\SphinxLiteralBlockLabel{\label{olap:point-cuts}}
\SphinxSetupCaptionForVerbatim{literal-block}{Point cut example}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{cuts} \PYG{o}{=} \PYG{p}{[}
   \PYG{n}{PointCut}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{geography}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{united states of america}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
   \PYG{n}{PointCut}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2010}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2012}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{)}
   \PYG{p}{]}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

A range might look like this:

\def\SphinxLiteralBlockLabel{\label{olap:range-cuts}}
\SphinxSetupCaptionForVerbatim{literal-block}{Range cut example}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{cuts} \PYG{o}{=} \PYG{p}{[}
    \PYG{n}{PointCut}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2010}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2012}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{24}\PYG{p}{]}\PYG{p}{)}
\PYG{p}{]}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\subsection{Aggregate}
\label{olap:aggregate}
Aggregate of a cell:

\def\SphinxLiteralBlockLabel{\label{olap:cell-aggregate}}
\SphinxSetupCaptionForVerbatim{literal-block}{Aggregate of a Cell}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{n}{cuts} \PYG{o}{=} \PYG{p}{[}
     \PYG{n}{PointCut}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{geography}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sk}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
     \PYG{n}{PointCut}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2010}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2012}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
 \PYG{p}{]}
 \PYG{n}{cell} \PYG{o}{=} \PYG{n}{Cell}\PYG{p}{(}\PYG{n}{cube}\PYG{p}{,} \PYG{n}{cuts}\PYG{p}{)}
 \PYG{n}{result} \PYG{o}{=} \PYG{n}{browser}\PYG{o}{.}\PYG{n}{aggregate}\PYG{p}{(}\PYG{n}{cell}\PYG{p}{)}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

We have a situation when a different hierarchy is desired than the default, so we have defined the hierarchy we wanted:

\def\SphinxLiteralBlockLabel{\label{olap:cell-hierachy}}
\SphinxSetupCaptionForVerbatim{literal-block}{Aggregate of a Cell with hierarchy}
\begin{Verbatim}[commandchars=\\\{\}]
 \PYG{n}{cuts} \PYG{o}{=} \PYG{p}{[}
     \PYG{n}{PointCut}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2010}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2012}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{hierarchy}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{yqmd}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
 \PYG{p}{]}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\subsection{Drilldown}
\label{olap:drilldown}
Drill-down – get more details, group the aggregation by dimension members.
For example:

\def\SphinxLiteralBlockLabel{\label{olap:id10}}
\SphinxSetupCaptionForVerbatim{literal-block}{Drilldown}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{cut} \PYG{o}{=} \PYG{n}{PointCut}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2010}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{cell} \PYG{o}{=} \PYG{n}{Cell}\PYG{p}{(}\PYG{n}{cube}\PYG{p}{,} \PYG{p}{[}\PYG{n}{cut}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{browser}\PYG{o}{.}\PYG{n}{aggregate}\PYG{p}{(}\PYG{n}{cell}\PYG{p}{,} \PYG{n}{drilldown}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

\textbf{Implicit}

The way we are drilling down, is that cubes knows the next level of the drilled dimension.
We have stated that our geography dimension has 4 levels: country, state, city, zip. This
means that the first level of the dimension is country. The “next level” is determined as the next
level after the deepest level used in a cut. The next level is by state, and so on. If the cut is at
its deepest level (zip), it is not possible to to drill-down deeper.

\textbf{Explicit}

For explicit drilling down, the cut is not considered for the drill down level. We have just implicitly declared the drill-down levels.


\section{Drill Down Tree}
\label{olap:drill-down-tree}
We have created a function that traverse a dimension hierarchy and print-out aggregations
(count of transactions and award amounts) at the actual browsed location.

Attributes:
\begin{itemize}
\item {} 
dimension - dimension to be traversed trough all levels.

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{drilldown}\PYG{p}{(}\PYG{n}{dim\PYGZus{}name}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{)}\PYG{p}{:}
\end{Verbatim}

First we need to get the dimension hierarchy to know the order of levels. Most dimensions have only one hierarchy, thought.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{dimension} \PYG{o}{=} \PYG{n}{cube}\PYG{o}{.}\PYG{n}{dimension}\PYG{p}{(}\PYG{n}{dim\PYGZus{}name}\PYG{p}{)}
\PYG{n}{hierarchy} \PYG{o}{=} \PYG{n}{dimension}\PYG{o}{.}\PYG{n}{hierarchy}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}

Parser the cut request parameter and convert it to a list of actual cube cuts.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{cutstr} \PYG{o}{=} \PYG{n}{request}\PYG{o}{.}\PYG{n}{args}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cut}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{hierarchy} \PYG{o}{=} \PYG{n}{dimension}\PYG{o}{.}\PYG{n}{hierarchy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{cell} \PYG{o}{=} \PYG{n}{cubes}\PYG{o}{.}\PYG{n}{Cell}\PYG{p}{(}\PYG{n}{browser}\PYG{o}{.}\PYG{n}{cube}\PYG{p}{,} \PYG{n}{cubes}\PYG{o}{.}\PYG{n}{cuts\PYGZus{}from\PYGZus{}string}\PYG{p}{(}\PYG{n}{cube}\PYG{p}{,} \PYG{n}{cutstr}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

We get the cut of actually browsed dimension, so we know “where we are”.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{cut} \PYG{o}{=} \PYG{n}{cell}\PYG{o}{.}\PYG{n}{cut\PYGZus{}for\PYGZus{}dimension}\PYG{p}{(}\PYG{n}{dimension}\PYG{p}{)}

\PYG{k}{if} \PYG{n}{cut}\PYG{p}{:}
    \PYG{n}{path} \PYG{o}{=} \PYG{n}{cut}\PYG{o}{.}\PYG{n}{path}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{path} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\end{Verbatim}

Now we do the actual aggregation of the cell.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{levels} \PYG{o}{=} \PYG{n}{hierarchy}\PYG{o}{.}\PYG{n}{levels\PYGZus{}for\PYGZus{}path}\PYG{p}{(}\PYG{n}{path}\PYG{p}{)}
\PYG{k}{if} \PYG{n}{levels}\PYG{p}{:}
    \PYG{n}{next\PYGZus{}level} \PYG{o}{=} \PYG{n}{hierarchy}\PYG{o}{.}\PYG{n}{next\PYGZus{}level}\PYG{p}{(}\PYG{n}{levels}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{next\PYGZus{}level} \PYG{o}{=} \PYG{n}{hierarchy}\PYG{o}{.}\PYG{n}{next\PYGZus{}level}\PYG{p}{(}\PYG{n+nb+bp}{None}\PYG{p}{)}
\end{Verbatim}

We have created a flag, so that we know that we are at the deepest level.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{is\PYGZus{}last} \PYG{o}{=} \PYG{n}{hierarchy}\PYG{o}{.}\PYG{n}{is\PYGZus{}last}\PYG{p}{(}\PYG{n}{next\PYGZus{}level}\PYG{p}{)}
\end{Verbatim}

And finally we, render the template

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{return} \PYG{n}{render\PYGZus{}template}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{drilldown.html}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{dimensions}\PYG{o}{=}\PYG{n}{cube}\PYG{o}{.}\PYG{n}{dimensions}\PYG{p}{,} \PYG{n}{dimension}\PYG{o}{=}\PYG{n}{dimension}\PYG{p}{,}
                        \PYG{n}{levels}\PYG{o}{=}\PYG{n}{levels}\PYG{p}{,} \PYG{n}{next\PYGZus{}level}\PYG{o}{=}\PYG{n}{next\PYGZus{}level}\PYG{p}{,} \PYG{n}{result}\PYG{o}{=}\PYG{n}{result}\PYG{p}{,}
                        \PYG{n}{cell}\PYG{o}{=}\PYG{n}{cell}\PYG{p}{,} \PYG{n}{is\PYGZus{}last}\PYG{o}{=}\PYG{n}{is\PYGZus{}last}\PYG{p}{,} \PYG{n}{details}\PYG{o}{=}\PYG{n}{details}\PYG{p}{,} \PYG{n}{pagination}\PYG{o}{=}\PYG{n}{pagination}
                        \PYG{p}{)}
\end{Verbatim}


\chapter{Web Application}
\label{olap:web-application}\begin{figure}[h]
\centering
\capstart

\scalebox{0.800000}{\includegraphics{{website}.png}}
\caption{Opengovernment - web application}\label{olap:id24}\end{figure}

The final major component of the implementation environment is the web application.
We have build a web application that provide variety of capabilities to general users to leverage
the presentation area for analytical decision making.

For the web development of the application we have used a micro web development framework for Python called Flask.
For the front end we have used bootstrap framework to design the layouts and the interface. We are also relying
on some  javascript libraries for the charts.

The open government folder is where we have dropped our files. We have put directly in this folder our
data warehouse schema, models for cubes, as well as the main application module. The static folder is a
place where we have placed css, javascript files. Inside the templates folder the application looks for the templates.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{o}{/}\PYG{n}{opengovernment}
   \PYG{o}{/}\PYG{n}{static}
   \PYG{o}{/}\PYG{n}{templates}
   \PYG{o}{/}\PYG{n}{postgres}
   \PYG{o}{/}\PYG{n}{models}
   \PYG{n}{flask\PYGZus{}app}\PYG{o}{.}\PYG{n}{py}
\end{Verbatim}

The actual application module, we have named it flask\_app.py is the core of the back end.
This is where the all web development functions are implemented. We have integrated Cubes Slicer
Server with the application to provide raw analytical data. We have provided Slicer as a flask Blueprint
to the application - a module that is plugged in.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{n}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} Create a Slicer and register it at http://localhost:5000/slicer}
    \PYG{n}{app}\PYG{o}{.}\PYG{n}{register\PYGZus{}blueprint}\PYG{p}{(}\PYG{n}{slicer}\PYG{p}{,} \PYG{n}{url\PYGZus{}prefix}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/slicer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{config}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{slicer.ini}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{app}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{n}{host}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{localhost}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{port}\PYG{o}{=}\PYG{l+m+mi}{8000}\PYG{p}{,} \PYG{n}{debug}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}
\end{Verbatim}

We have implemented the following 7 features in our web application:
\begin{itemize}
\item {} 
\textbf{Spending} - A good place to start to find information about federal funding.
On Spending Page users just have to input a zip code which they are interested in and
see all the federal transactions in their backyard. They can also see the transactions throughout state,
county, as well as by federal agency and award type.

\item {} 
\textbf{Dashboard} - On Dashboard Page we enabled users to drill down from one place to another,
they will find information to detailed data by focusing on something. For example to drill down through
a series of place where the work was/is performed.

\item {} 
\textbf{Reports} - We have implemented a visual tool for exploring and analysing our OLAP Cube.
We  have enabled users to explore data, create their own reports. It enables data exploration,
data auditory, generation of reports, chart design and simple analytics.

\item {} 
\textbf{Charts} - We have also created our own charts using javascript libraries.
On Chart pages users can drill down/roll up donut charts. Or they can look at Agencies
Profile to see the total amount of money an agency awarded or State Summary to find out
the total dollar amount for federal transactions for cities where the money was distributed.

\end{itemize}


\chapter{Conclusion}
\label{conclusion::doc}\label{conclusion:conclusion}
The bachelor thesis was aimed to implement an open-data data warehouse using open government data.
As we have been started through searching for data, we have studied the metadata of the source, the
process in detail and observe the transactions of government institutes, such as contracts, grants,
loans and other financial assistance, we have decided that the federal award transaction is thebusiness process to be modelled.

Once the business model has been identified, we faced a serious decision about the granularity of the data warehouse.
We ensured maximum dimensionality and flexibility by choosing the most granular data - individual transactions made by federal agencies.
After we have declared the grain of the fact table, the dimensions fall out immediately.

We have made various transformations on data to enforce data quality, integrity and consistency, conformed data so
that separate flat files can be used together, and delivered data into the presentation layer.

Then, we have designed the multidimensional model of the presentation layer, which is based on online analytic processing
(OLAP) technology, so that the data is stored in cubes. We have properly specified the analytical workspace of the Cubes.
Following we have provided logical model metadata to the workspace.

Finally, the major component of the the implementation was to make it accessible and available to the end users.
For this reason, we have build a web application using a micro web development framework for Python called Flask.
We have implemented various features for the end users. Visual tool for exploring and analysis, which enables data
exploration, data auditory, generation of reports, chart design and simple analytics. By this we have enabled the
general public to evaluate the business process of the government.


\chapter{Resume in Slovak language}
\label{resume::doc}\label{resume:resume-in-slovak-language}

\section{Otvorené Dáta}
\label{resume:otvorene-data}
Viete aká časť z vašich daní ide na verejne osvetlenie či na verejnú dopravu? Zamysleli ste sa
niekedy či do vášho regiónu investujú nejaké inštitúcie, prípadne koľko prostriedkov takéto inštitúcie
investujú? Nové technológie v súčasnosti poskytujú odpovede na takýto typ otázok.

Väčšinu dát na odpovedanie týchto otázok generujú samotné verejné inštitúcie. Avšak častokrát požadované
dáta nie sú k dispozícii vo forme výstupe, ktorý by bol jednoduchý a účelný. Ak si vezmeme ako príklad našu krajinu,
Slovensko, môžeme vidieť, že tu stále chýba transparentosť informácií a takisto tvorenie dátových setov, ktoré by boli
užívateľovi jednoducho prístupné.

Čo sú to otvorené dáta? Otvorené dáta sú dáta, ktoré môžu byť voľne použité a redistrubuované kýmkoľvek.
Dáta musia byť dostupné vo vhodnej, modifikovateľnej forme. Otvorené dáta a najmä otvorené dáta týkajúce sa
verejnej správy sú oblasťou, ktorá je stále nevyužitá a jej potenciál sa nenapĺňa. V súčasnosti je možné
poukázať na určité oblasti, kde otvorené dáta týkajúce sa verejnej správy tvoria hodnoty. Niektoré z týchto oblastí zahŕňajú:
\begin{itemize}
\item {} 
Transparentnosť a kontrola verejnosťou

\item {} 
Seba posilnenie

\item {} 
Zlepšenie produktov a služieb

\item {} 
Inovácie

\item {} 
Zlepšenie užitočnosti a efektivity štátnych služieb

\end{itemize}

Prvým krokom naštartovania tohto nevyužitého potenciálu môže byť transformácia dát štátnej správy na otvorené dáta.
Toto sa môže uskutočniť len za podmienok, že sú skutočne otvorené a že neexistujú žiadne obmedzenia (právne, finančné
alebo technologické). Z tohto vyplýva, že na realizáciu a využitie potenciálu dát, dáta musia mať charakter otvorených dát.


\section{Základy dátového skladu}
\label{resume:zaklady-datoveho-skladu}

\subsection{Co je to datovy sklad?}
\label{resume:co-je-to-datovy-sklad}
Dátový sklad (DW) je systém na reportovanie a analýzu dát. Dátové sklady sú centrálnym úložiskom integrovaných dát
z jedného alebo z viacerých rôznych zdrojov. Ukladajú aktuálne aj staršie dáta a používajú sa pre vytvorenie analytických
správ slúžiacich pre pracovníkov celej spoločnosti.


\subsection{Operačný systém}
\label{resume:operacny-system}
Operačný systém priamo podporuje realizáciu business procesu tým, že zachytí detaily o signifikantných udalostiach
alebo transakciách. Napríklad predajný systém,  zhromažďuje informácie o objednávkach, zásielkach či reklamáciach.


\subsection{Analytický systém}
\label{resume:analyticky-system}
Analytický systém podporuje vyhodnotenie obchodného procesu. Akým trendom sa uberajú objednávky tento mesiac v porovnaní s minulým?
Kde sa ocitáme pri porovnávaní našich cieľov predaja v rámci kvartálu?


\subsection{Analytické databázy a Dimenzionálny navrh}
\label{resume:analyticke-databazy-a-dimenzionalny-navrh}
Dimenzionálny model business processu sa skladá z dvoch častí a to:  z rozmerov a ich kontextu.
Je známe, že tieto komponeny sú usporiadané do návrhu databázy, ktorá poskytuje širokú škálu analytického využitia. Realizovanie v multidimenzionálnej databáze, je známe pod pojmom kocka.

Analytické systémy a operačné systémy slúžia pre zásadne odlišné účely. Operačný systém podporuje realizáciu obchodného procesu, zatiaľ čo analytický systém zabezpečuje vyhodnocovanie procesu.

Každé dimenzionálne riešenie popisuje proces tým, že zachytí to, čo je merané a kontext v ktorom sú merania vyhodnocované.


\section{Návrh Dátového skladu}
\label{resume:navrh-datoveho-skladu}

\subsection{Cieľe Dátového skladu}
\label{resume:ciele-datoveho-skladu}
Pred tým ako sme sa zamerali na dimenzionálne modelovanie a implementáciu, bolo potrebné sa zamerať na základné
ciele dátového skladu. Ako je možné sa zamerať na tieto základné ciele, ak nám chýba to najdôležitejšie a síce, dáta.
Na základe našich skúseností môžeme tvrdiť, že dáta sú univerzum a tvoria základné požiadavky resp. sú stavebným základom
pre dátový sklad. Dáta sú vždy na prvom mieste pred technológiami a business modelom.


\subsection{Hľadanie dát}
\label{resume:hladanie-dat}
Rozhodli sme sa zamerať na dáta týkajúce sa štátnej správy. Skúmali sme niekoľko dátových sád a napokon sme vybrali „Usa Spending Gov“.

Všetky údaje transakcie príjemcu na USAspending.gov sú reportované federálnymi agentúrami, ktoré uzatvárajú kontrakty,
úvery, granty a dalšie obstarávania. Po identifikácii zdroja dát a preskúmaní súborov dát sme sa rozhodli použiť tieto
dáta pre náš business model a pre model dátového skladu.

Pre potrebny nášho dátoveho skladu sme si stanovili nasledujúce ciele:
\begin{itemize}
\item {} 
Hlavným poslaním dátového skladu bolo zverejnenie dát federálnych organizácií. Kľúčom úspechu a víziou nášho dátového skladu bolo, či dátový sklad účinne slúži širokej verejnosti.

\item {} 
Dátový sklad musí sprístupniť informácie ľahko a dostupne.

\item {} 
Takisto musí prezentovať informácie federálnych organizácií konzistentne.

\item {} 
Dátový sklad musí byť prispôsobivý a odolné voči zmene.

\item {} 
Je potrebné najskôr chápať jednotlivé časťi dátového skladu pred tým ako ich začneme kombinovať.

\end{itemize}


\chapter{References}
\label{references::doc}\label{references:references}
\phantomsection\label{references:id1}{[}1{]} Authors: School of Data, Organization: School of Data Date: Sep 02, 2013 Available from: \href{http://schoolofdata.org/handbook/courses/what-is-data/}{Data Fundamentals}




\chapter{Appendix A}
\label{apendixa::doc}\label{apendixa:appendix-a}

\section{Instalation Manual}
\label{apendixa:instalation-manual}
In this section of appendinx, user guide to setting up the environment and installation is described.
This sections covers all the pre-requireties and procedures for demonstration of the bachelor thesis.


\subsection{Python}
\label{apendixa:python}
Before we start, we will need Python on your computer, but you may not need to download it.
First of all check that you don't already have Python installed by entering python in a command line window.
If you see a response from a Python interpreter it will include a version number in its initial display.

\def\SphinxLiteralBlockLabel{\label{apendixa:python-in-command-line}}
\SphinxSetupCaptionForVerbatim{literal-block}{Python in command line}
\begin{Verbatim}[commandchars=\\\{\}]
Python 3.4.3 \PYG{o}{(}v3.4.3:9b73f1c3e601, Feb \PYG{l+m}{23} 2015, 02:52:03\PYG{o}{)}
\PYG{o}{[}GCC 4.2.1 \PYG{o}{(}Apple Inc. build 5666\PYG{o}{)} \PYG{o}{(}dot 3\PYG{o}{)}\PYG{o}{]} on darwin
Type \PYG{l+s+s2}{\PYGZdq{}help\PYGZdq{}}, \PYG{l+s+s2}{\PYGZdq{}copyright\PYGZdq{}}, \PYG{l+s+s2}{\PYGZdq{}credits\PYGZdq{}} or \PYG{l+s+s2}{\PYGZdq{}license\PYGZdq{}} \PYG{k}{for} more information.
\PYGZgt{}\PYGZgt{}\PYGZgt{}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

If you need to install Python, you may as well download the most recent stable version Python 3.

\textbf{If you're running Windows:} the most stable Windows downloads are available from the \titleref{https://www.python.org/downloads/windows/}.

\textbf{If you are using a Mac:} see the \titleref{https://www.python.org/downloads/windows/}.

\textbf{For Debian or Ubuntu:} For Debian or Ubuntu, install the python3.x and python3.x-dev packages


\subsection{Install Packages}
\label{apendixa:install-packages}
This section covers the basics of how to install Python packages (i.e. a bundle of software to be installed).

If you have Python 3 \textgreater{}=3.4 installed from python.org, you will already have pip and setuptools,
but will need to upgrade to the latest version.

On Linux or OS X:

\def\SphinxLiteralBlockLabel{\label{apendixa:pip-install}}
\SphinxSetupCaptionForVerbatim{literal-block}{Pip install}
\begin{Verbatim}[commandchars=\\\{\}]
pip install \PYGZhy{}U
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

On Windows:

\def\SphinxLiteralBlockLabel{\label{apendixa:pip-install-on-windows}}
\SphinxSetupCaptionForVerbatim{literal-block}{Pip install on Windows}
\begin{Verbatim}[commandchars=\\\{\}]
python \PYGZhy{}m pip install
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\subsection{Creating Virtual Environments}
\label{apendixa:creating-virtual-environments}
Python “Virtual Environments” allow Python packages to be installed in an isolated location for
a particular application, rather than being installed globally.

The basic commands:

\def\SphinxLiteralBlockLabel{\label{apendixa:creating-virtual-environment}}
\SphinxSetupCaptionForVerbatim{literal-block}{Creating Virtual Environment}
\begin{Verbatim}[commandchars=\\\{\}]
pip install virtualenv
virtualenv \PYGZlt{}DIR\PYGZgt{}
\PYG{n+nb}{source} \PYGZlt{}DIR\PYGZgt{}/bin/activate
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\subsection{PostgreSQL}
\label{apendixa:postgresql}
PostgreSQL is an object-relational database management system (ORDBMS).
It supports a large part of the SQL standard and offers many modern features.

\textbf{For Windows distribution:} \titleref{http://www.enterprisedb.com/products-services-training/pgdownload}

After downloading the source installation just simple run the setup.exe and follow the instructions.

\textbf{For OS X distribution:} \titleref{http://postgresapp.com}

For MAC OS X we recommend to download Postgres.app which is a simple application that runs a menubar without the need of any installation.


\subsection{Cubes}
\label{apendixa:cubes}
You may install Cubes with the minimal dependencies:

\def\SphinxLiteralBlockLabel{\label{apendixa:install-cubes}}
\SphinxSetupCaptionForVerbatim{literal-block}{Install Cubes}
\begin{Verbatim}[commandchars=\\\{\}]
pip install cubes
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

The cubes has optional requirements:
\begin{itemize}
\item {} 
\textbf{SQLAlchemy} for SQL database

\item {} 
\textbf{Flask} for Slicer OLAP HTTP server

\end{itemize}

To install these requirements:

\def\SphinxLiteralBlockLabel{\label{apendixa:cubes-requirements}}
\SphinxSetupCaptionForVerbatim{literal-block}{Cubes requirements}
\begin{Verbatim}[commandchars=\\\{\}]
pip install Flask SQLAlchemy
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

There is also a customized installation with requirements:

\def\SphinxLiteralBlockLabel{\label{apendixa:customized-installation}}
\SphinxSetupCaptionForVerbatim{literal-block}{Customized installation}
\begin{Verbatim}[commandchars=\\\{\}]
pip install python\PYGZhy{}dateutil jsonschema expressions grako click
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

To run Cubes OLAP HTTP server:

\def\SphinxLiteralBlockLabel{\label{apendixa:olap-http-server}}
\SphinxSetupCaptionForVerbatim{literal-block}{OLAP HTTP server}
\begin{Verbatim}[commandchars=\\\{\}]
slicer serve slicer.ini
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

And try to do some queries:

\def\SphinxLiteralBlockLabel{\label{apendixa:id1}}
\SphinxSetupCaptionForVerbatim{literal-block}{OLAP HTTP server}
\begin{Verbatim}[commandchars=\\\{\}]
curl \PYG{l+s+s2}{\PYGZdq{}http://localhost:5000/cube/spending/aggregate\PYGZdq{}}
curl \PYG{l+s+s2}{\PYGZdq{}http://localhost:5000/cube/spending/aggregate?drilldown=year\PYGZdq{}}
curl \PYG{l+s+s2}{\PYGZdq{}http://localhost:5000/cube/spending/aggregate?drilldown=country\PYGZdq{}}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\subsection{Flask}
\label{apendixa:flask}
In order to run the web application you have install Flask which is a micro
webdevelopment framework for Python.

Enter the following command to get Flask activated in your virtualenv

\def\SphinxLiteralBlockLabel{\label{apendixa:install-flask}}
\SphinxSetupCaptionForVerbatim{literal-block}{Install Flask}
\begin{Verbatim}[commandchars=\\\{\}]
pip install Flask
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

Flask depends on two external libraries Werkzeug and Jinja2.
The web application depends also on other external packages. For successfuly running the application
install the following packages

\def\SphinxLiteralBlockLabel{\label{apendixa:install-external-packages}}
\SphinxSetupCaptionForVerbatim{literal-block}{Install external packages}
\begin{Verbatim}[commandchars=\\\{\}]
pip install Werkzeug jinja2 Flask\PYGZhy{}WTF
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\subsection{Web application}
\label{apendixa:web-application}
After all these installations, you should be able to run the application. Now just copy the \emph{opengovernment} folder,
which is located on the DVD, into your home directory of your virtual environment.
Open the \emph{postgres} folder and run the following command to load the data into database

\def\SphinxLiteralBlockLabel{\label{apendixa:load-data}}
\SphinxSetupCaptionForVerbatim{literal-block}{Load Data}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{pg\PYGZus{}restore} \PYG{o}{\PYGZhy{}}\PYG{n}{Fc} \PYG{o}{\PYGZhy{}}\PYG{k}{C} \PYG{n}{pg\PYGZus{}dump}\PYG{p}{.}\PYG{n}{gz}
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

Move up to folder \emph{opengovernment}. Run the Cubes OLAP HTTP server.

\def\SphinxLiteralBlockLabel{\label{apendixa:id2}}
\SphinxSetupCaptionForVerbatim{literal-block}{OLAP HTTP server}
\begin{Verbatim}[commandchars=\\\{\}]
slicer serve slicer.ini
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty

Now run the actual Flask application \emph{flask\_app.py}

\def\SphinxLiteralBlockLabel{\label{apendixa:run-flask-app}}
\SphinxSetupCaptionForVerbatim{literal-block}{Run Flask App}
\begin{Verbatim}[commandchars=\\\{\}]
python flask\PYGZus{}app.py
\end{Verbatim}
\let\SphinxVerbatimTitle\empty
\let\SphinxLiteralBlockLabel\empty


\chapter{Appendix B}
\label{appendixb::doc}\label{appendixb:appendix-b}

\section{Contents of DVD}
\label{appendixb:contents-of-dvd}
The content of DVD includes the following files:
\begin{itemize}
\item {} 
\textbf{/opengovernment/} the main directory which contains all the source code of the bachelor thesis.

\item {} 
\textbf{/opengovernment/static/} folder contains all the static files for the web application such as css, javascript, images.

\item {} 
\textbf{/opengovernment/data/} folder contains the source data and the cleaned data.

\item {} 
\textbf{/opengovernment/postgres/} contains dump of example data and schema for Postgres database also the ETL jobs.

\item {} 
\textbf{/opengovernment/models/} model metadata

\item {} 
\textbf{/opengovernment/docs/html/index.html} documentation of the bachelor thesis in html format.

\item {} 
\textbf{/opengovernment/slicer.ini} OLAP HTTP WSGI Server configuration file.

\item {} 
\textbf{/opengovernment/tempaltes/} templates for the web application.

\item {} 
\textbf{/opengovernment/flask\_app} the main web application modul.

\end{itemize}

\begin{thebibliography}{workspace}
\bibitem[Cubes]{Cubes}{\phantomsection\label{olap:cubes} 
Author: Stefan Urbanek
Availailable: \href{http://cubes.readthedocs.io/en/v1.0.1/\#}{Cubes Overview}
}
\bibitem[workspace]{workspace}{\phantomsection\label{olap:workspace} 
Author: Stefan Urbanek
Availailable: \href{http://cubes.readthedocs.io/en/v1.0.1/tutorial.html}{Cubes Analytical Workspace}
}
\bibitem[model]{model}{\phantomsection\label{olap:model} 
Author: Stefan Urbanek
Availailable: \href{http://cubes.readthedocs.io/en/v1.0.1/model.html}{Cubes Model}
}
\bibitem[cells]{cells}{\phantomsection\label{olap:cells} 
Author: Stefan Urbanek
Availailable: \href{http://cubes.readthedocs.io/en/v1.0.1/slicing\_and\_dicing.html}{Slicing and Dicing}
}
\end{thebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}
