Building The Data Warehouse
===========================


Plan
----

Goals of Data Warehouse
"""""""""""""""""""""""

Before we delve into the details of dimensional modeling and implementation, it is helpful
to focus on fundamental goals of the data warehouse. How can we focus on these
these fundamental goals if we are missing the most important thing, the **data**.

Based on our experience, the data is the universum that drive the bedrock requirements for
the data warehouse. The data comes first, than the technology and the bussiness model.

Finding the Data
################

After couple of weeks of searching we have identified several open to public data sources:

* `UK Gov <https://www.gov.uk>`_
* `Open Spending <https://openspending.org>`_
* `U.S. Government’s open data <https://www.data.gov>`_
* `Usa Spending Gov <https://www.usaspending.gov/Pages/Default.aspx>`_

We have decided to looking for government data. We exemined couple of datasets and
we picked `Usa Spending Gov <https://www.usaspending.gov/Pages/Default.aspx>`_.
Their data looked the most promicing and they provide great
`document <https://www.usaspending.gov/about/PublishingImages/Pages/TheData/USAspending.gov%20Data%20Dictionary.pdf>`_
information about the data fields, description and their formats.

**Identifying the data source**

All the prime recipient transaction data on `USAspending.gov <https://www.usaspending.gov>`
is reported by the federal agencies making contract, grant, loan, and other financial assistance awards.
After identifying the data source and examination of the data sets we have decided to use this data
for our business model and data warehouse model.

**Mission of data warehouse**

We have concluded the following goals for our data warehouse:

**The main mission of the data warehouse is to publish the federal organisations data.**
The key success of our data warehouse is whether the data warehouse effectively contributes to the general public.
The success of a data warehouse begins end ends with its users.

The data warehouse is going to be open to public. **It must make the information easily accessible.**
The content of the data warehouse must be understandable. The data must be intuitive and obvious to the business user,
not merely the developer.

**The data warehouse must present the federal organisations information consistently.**
Data must be carefully assembled from a variety of sources around the organization, cleansed, quality assured,
and released only when it is fit for user consumption.

**The data warehouse must be adaptive and resilient to change.**
We want to track changes of federal awards made by federal agencies.
The data warehouse must be designed to handle this change.

Data warehouse Environment
--------------------------
It is helpful to understand the pieces of the data warehouse before we begin to combine them.
Each component serves specific function.


.. figure:: images/dw_pieces.jpg
   :scale: 100 %

   Data warehouse components

Operational Source systems
""""""""""""""""""""""""""

The  operational system directly captures the execution of a business process,
in our case capture the transactions of federal agencies. The operational source system is outside the data warehouse,
we have no control over the content and format of the data in these systems. In our case the operational system is
`usaspending.gov <https://www.usaspending.gov>`_ where we just download directly from they site a .csv flat file.

Data Staging Area
"""""""""""""""""

The data staging is a physical storage area and a set of extract-transformation-load (ETL) jobs.
The data staging are is everything between the source systems and the data presentation area.
This is the stage where we perform various process on the data to fit into data warehouse environment.

Data presentation
"""""""""""""""""

The data presentation area is where the structured data is organized, stored,
and made available for querying and for analytical applications. The presentation area is based
on online analytic processing (OLAP) technology, this means the data is stored in cubes.

Data access tools
"""""""""""""""""

The final piece of the data warehouse is the data access tool. Obviously the first access tool
can be a simple query tool for querying.  We provide to our end users the general public set of
tools for development of reporting, analysis and browsing of data trough our web application using the concept of Cubes.


Data Staging Area
-----------------

The ETL system is the foundation of the data warehouse. We have design an ETL system that extracts data from the source
system, enforces data quality and consistency, conforms data so that separate flat files can be used together,
and delivers data into presentation layer where we build the application so that end users can make decisions. We haven’t
used any ETL tools every process/job is all hand coded.


.. figure:: images/etl.png
   :scale: 90 %

   ETL process flow

Extract
"""""""

The `usaspending.gov <https://www.usaspending.gov>`_  website doesn’t provide any API, only single page with the download able link.
The raw data coming from the source system is stored locally on the disk. We have downloaded 4 different .csv flat files.


.. figure:: images/e.png
   :scale: 100 %

   Figure [5]

We have examined the .csv flat files. We have encountered various errors when we wanted to load the data into our database.
Here is an example of a source flat file:

.. code-block:: html
    :caption: Example of raw data
    :name: Raw Data

    "07: Direct loan","2",": Current record","","4967000","UTAH","SALT, LAKE”,"USA","841062671",  “”,

The empty double quotes PostgreSQL treats as empty string and not as NULL value. When we wanted to insert the data into
the database we have always encountered syntax type error for numeric types. We have decided to get rid of the double
quotes in the flat files. For that reason we have made a little Python script which strip all the double quotes,
in a way that we haven’t disturb data integrity.

.. code-block:: python
    :caption: Script to strip double quotes
    :name: Python Script - Strip double quotes

    for file in csv_files:
      source = open(file, 'r')
      source_name = os.path.basename(file).split('_')
      cleaned_csv = open(path_cleaned+source_name[0]+'_cleaned.csv', 'w')
      reader = csv.reader(source)
      writer = csv.writer(cleaned_csv)
      for row in reader:
          writer.writerow(row)
      source.close()
      cleaned_csv.close()

After we have stripped all the double quotes from all  the flat files we have managed to load the data into database.
All the tables start with a prefix *base_* + name of the flat file from the source. For example for grants: *base_grants* table.

**PICTURE**

After we have created the base tables where we have loaded the extracted data from the source.
We started with the highest level of business objectives, identify the likely data from the base tables,
that we believe will support the decisions needed by the business model - which we have documented in the 
Dimensional Modelling sections. We started with identifying specific data elements we believed are the most 
important for the business model. We analysed the data in the base tables and come to conclusion that data in these 
tables must be examined for data quality, completeness, and fitness for the purpose of the data warehouse. For the simplicity 
we have designed logical data mapping document. The document contains the data from the source systems throughout to the target
data table, and the exact manipulation of the data required to transform it from its original format to that of its target table.

**TABLE**

Clean
"""""

After we have populated our target table, we have created a various jobs to clean the data from the source and consolidate them.
Staging table contains mostly raw text values and numeric only for amounts. Content of the table mostly matches information fromthe source.

Goal of these jobs more specifically:

* Cleanse DUNS number, zip code
* Cleanse all columns of unwanted special characters (+,#,@,!)
* Replace all foreign entities with a FRGN flag, there are lot of cases when the place of performance of agreements has been outside of United States of America. In most cases these values where mostly incorrect or have been unknown.
* Correct state names. There are cases when the state names for United States of America are given only by their abbreviations.
* Geography consolidation

**Geography Consolidation**

Table with geography country names sometime contain only their country codes. For those cases a mapping table have been
created where we have specified a mapping of country codes to their valid country names.

The process is depicted in the following image:

**IMAGE**

Load
""""

After extracting, cleaning and transformation finally it’s time to populate our dimension tables and fact table.
First we have created the dimensions, we assign every dimension table a surrogate key, we have added a EXCEPT
statement into our INSERT statement for a reason of avoiding duplicates to be inserted into the dimension tables.

This step consisted of the following creates and loads of all structures for analytical processing.

* fact table - fact is transcation
* dimensions:
  * geography
  * agency
  * recipient
  * date
  * type of transactions
  * award

**Dimensions**

Examples how the dimensions have been created:

**Geogprahy Dimension**

.. code-block:: sql
   :caption: Geography Dimension - SQL
   :name: Create Geography Dimension

    CREATE TABLE dm_geography (
        id SERIAL PRIMARY KEY,
        state text,
        city text,
        zip text,
        country text
    );

**Recipeint Dimension**

.. code-block:: sql
   :caption: Recipient Dimension - SQL
   :name: Create Recipient Dimension - SQL

    CREATE TABLE dm_recipient (
        id SERIAL PRIMARY KEY,
        name text,
        streetaddress text,
        state text,
        city text,
        zip text,
        country text,
        duns text
    );

**Fact Table**

The initial situation was that we should have built a data warehouse, which is able archive large data by monthly basis.
We have also known that the information is needed for various reports on weekly, monthly, three months periods, and so on.
For this purpose of better query performance we have broke down the fact table into partitions. PostgreSQL supports partitioning
via table inheritance [https://www.postgresql.org/docs/9.1/static/ddl-partitioning.html]. We have made the partitioning in a such
a way that every child table inherits from a single master table.

The partitioning have been implemented in the following way:

1. We created the master fact table

.. code-block:: sql
   :caption: Fact Table
   :name: Fact Table

   CREATE TABLE ft_spending_2 (
      id SERIAL PRIMARY KEY NOT NULL,
      transaction_type_id integer NOT NULL,
      date_id integer NOT NULL,
      product_id integer NOT NULL,
      geography_id integer NOT NULL,
      agency_id integer NOT NULL,
      recipient_id integer NOT NULL,
      award_id integer NOT NULL,
      transaction_id text,
      award_amount numeric,
      transactions integer,
      last_modified_date date,
      date_added date
  );

2. We created the child fact tables, which inherits the master fact table and added checks for dates, because we wanted to
   ensure that we have only right data on each partition.Partitions starts from 2015-01-01 and ends 2015-02-01. Each partition contains one month data.

.. code-block:: sql
   :caption: Child fact table
   :name: Child fact table

    CREATE TABLE ft_spending_2_15M01(
        PRIMARY KEY (id, last_modified_date),
        CHECK ( last_modified_date >= DATE '2015-01-01' and last_modified_date < DATE '2015-02-01')
    ) INHERITS (ft_spending_2);

3. We created indexes to child fact tables to speed up day field usage, because almost all queries are triggered on the date field.

.. code-block:: sql
   :caption: Child table indexes
   :name: Child fact table indexes

   CREATE INDEX idx_15M01 ON ft_spending_2_15M01 (last_modified_date);

4. We wanted our data warehouse to be able to say INSERT INTO *spending* and have the data be redirected into the appropriate partition table.
   We have arranged this by creating a suitable trigger function to the master table.

.. code-block:: sql
   :caption: Trigger Function
   :name: Trigger Function

    CREATE OR REPLACE FUNCTION ft_partition_trigger()
    RETURNS TRIGGER AS $$
    BEGIN
      IF
        (NEW.last_modified_date >= DATE '2016-01-01' and NEW.last_modified_date < DATE '2016-02-01') THEN
        INSERT INTO ft_spending_2_16M01 VALUES (NEW.*);
        ELSIF (NEW.last_modified_date >= DATE '2016-02-01' and NEW.last_modified_date < DATE '2016-03-01') THEN
        INSERT INTO ft_spending_2_16M02 VALUES (NEW.*);
        ELSIF (NEW.last_modified_date >= DATE '2016-03-01' and NEW.last_modified_date < DATE '2016-04-01') THEN
        INSERT INTO ft_spending_2_16M03 VALUES (NEW.*);
        ...
        ELSIF (NEW.last_modified_date >= DATE '2015-11-01' and NEW.last_modified_date < DATE '2015-12-01') THEN
        INSERT INTO ft_spending_2_15M11 VALUES (NEW.*);
        ELSIF (NEW.last_modified_date >= DATE '2015-12-01' and NEW.last_modified_date < DATE '2016-01-01') THEN
        INSERT INTO ft_spending_2_15M12 VALUES (NEW.*);
        ELSE
          RAISE EXCEPTION 'DATE OUT OF RANGE!';
      END IF;
      RETURN NULL;
    END;
    $$
    LANGUAGE plpgsql;


After creating the trigger function, we created a trigger which calls the trigger function.

.. code-block:: sql
   :caption: Trigger
   :name:

    CREATE TRIGGER insert_ft_spending_2
      BEFORE INSERT ON ft_spending_2
        FOR EACH ROW EXECUTE PROCEDURE ft_partition_trigger_2();

With all of these steps we ensured the our master fact table is available and all UPDATEs. INSERT’s, SELECT’s and DELETE’s goes to the right child tables by date.

Finally it was time to populating the fact table. The fact table has been created simply by transforming cleansed data and joining with prepared dimensions.


Dimensional Modeling
--------------------

Business Process
""""""""""""""""

The first step in the design is to decide what business process to model by understanding of
the business requirements with an understanding of the available data. In our open government case study,
the general public wants to better understand how the government spends money, what kind of transactions are
made in their neighbourhood. Thus our process is a transactional model. This transactional data will allow us
to analyse what kind of awards are made by federal agencies in which states on what days and to whom.
Brief description of business model that we’ll use in our case study to make dimension and fact tables
more understandable. Imagine a federal agency for example Department of Agriculture making a award for a
recipient 1901 Combine Group, LLC for a combine harvester in Texas on 2015. To summarise it WHICH federal
agency is awarding WHOM for WHAT and WHERE is the place of the performance of the transaction made.

Declare the Grain
"""""""""""""""""

Once the business process has been identified. we faced a serious decision about the granularity of the data warehouse.
What level of data detail should be made available in the dimensional model?
After identifying the data, we had couple of options to choose. We wanted tackling the data at it’s lowest level,
most atomic grain made the most sense. The more detailed and atomic the fact measurement, the more things we know for
sure about federal awards. In this regard, atomic data was the perfect match for the dimensional approach.
Atomic data provides the maximum analytic flexibility because it can be constrained and rolled up in every way possible.
In our case study, the most granular data is an individual transaction made by federal agencies. Because of this level of
grain we ensured maximum dimensionality and flexibility. Providing access to the transactions information gave us very
detailed look at federal award changes. For example, the end users want to see how many transaction were made for one
individual award or how the award has changed over period of time, if the agency made a modification to an award,
reduced a portion of the original award amount or made additional funding.None of them could have been answered if we wouldn't
elected the lowest granularity just the summarised data.

Choose the Dimensions
"""""""""""""""""""""

After we have declared the grain of the fact table, the recipient, agency, date, geography, award, dimensions fall out immediately.
We assume that the calendar date is the date when the award was signed.

In our case study we have decided on the following dimensions:

.. figure:: images/dw_schema_tbd.png
   :scale: 100 %

   Preliminary star schema.

Dimension tables are not in third normal form. A dimensional model serves a different purpose from ER model.
It wasn’t necessary to isolate repeating values in an environment that doesn't support transaction processing.
If we would have made additional normalisation within dimensions, we would end up with the schema that is referred as a snowflake.
We have encouraged to resist the urge to snowflake given our to primary design, ease of use and performance.

* Snowflaked tables makes for much more complex presentation.
* Database design will struggle with the complexity of the snowflaked schema.
* Numerous tables and joins usually translate into slower query performance.
* Minor disk space savings.
* Snowflaking slows down the user’s ability to browse within the dimension.

Dimension tables also contain key columns that uniquely identify something in an operational system.
These key columns are referred to as natural keys. The separation of surrogate keys and natural keys
allows the data warehouse to track changes, even if the originating operational system does not.


Identify Facts
""""""""""""""

At the core of a star schema is the fact table. In addition to presenting the facts, the fact table includes surrogate
keys that refer to each of the associated dimension tables. Each row in the fact table stores facts at a specific
level of detail of our grain that we have declared. Facts tend to be numeric in value. We have made the decision
that the award amount is going to be our fact measurement which will appear in our fact table. We have decided
to stored physically in the data warehouse only one fact the award mount, which is additive across all dimensions.

Suroggate Keys
""""""""""""""

In the star schema, each dimension table is given a surrogate key. This column is a unique identifier,
created exclusively for the data warehouse. The surrogate key is the primary key of the dimension table.
In our case, surrogate keys are randomly generated integers that are assigned sequentially when populate
dimension tables during the ETL process. For example, the first recipient record is assigned a recipient
surrogate key with the value of 1, the next recipient record is assigned recipient key with value 2, and
so forth. The surrogate keys serve to join the dimension tables to the fact table. One of the most important
reasons why are we using surrogate keys and doesn’t just rely on natural keys from the source system is to
support handling changes to dimension table attributes.


Dimensional Table Attributes
----------------------------

Date Dimension
""""""""""""""

We have started with the date dimension, which is nearly guaranteed to be in every data warehouse.
Unlike other dimensions, we could have build the date dimension in advance. We have put 10 - 20 years of rows representing
days in the table so that we can cover the history we have stored, as well as several years in the future.
From our data we know that first public awards come from the year 2008, so even 10 years worth of days is only about 3650 rows,
which is relatively small dimension table.

.. figure:: images/date_dim.png
   :scale: 80 %

   Date Dimension

Each column in the date dimension table is defined by the particular day that the row represents.
The full day columns represents the full date when the award have been signed in format YYYY-MM-DD it data type is date.
The year columns represents the fiscal year of the award and it is a singe integer type. This column is useful for slicing,
when we want filter a particular year, for example we want to show all the transactions made in the year of 2015.
The month column represents the month number in the year (1, 2, …, 12). The month name columns represents the name of the month
(January, February etc.). Similarly, as month we have a day column, which represents the day number in calendar month columns
starts with 1 at the beginning of each month an it is depending on the particular month. The day in the year columns is a single
integer which is representing the day number in the year, it starts from 1 and runs to 365, depending on what year is.
The weekday name column represents the name of the day when the award have been signed, such as Wednesday. It has string data type.
We have also included calendar week column. We have included quarter number (Q1,…, Q4). Columns like year, quarter, month, calendar week,
day all these integers support as simple date filters. Figure 2.5 illustrates several rows from a partial date dimension table.

.. figure:: images/date_dim_detail.png
   :scale: 80 %

   Sample of Date Dimension

Product Dimension
"""""""""""""""""

.. figure:: images/product_dim.png
   :scale: 80 %

   Product Dimension

The product dimension describes every product or service for a particular award.
While a typical reasonable product dimension table would have 50 or more descriptive attributes, in our data warehouse
we have only one product name attribute column. It is sourced from the operational system as a single attribute column with no hierarchy.
The only function of the product dimension is to give a descriptive attribute of each award. In our case there is only
2492 different values in the product dimension table. Viewed in this manner, we can only drill down on one level of the product
dimension which provides us information about the award amount and quantity by product name. For example here is simple
report overview, we have summarised the award amount and quantity (count) by product name.

.. figure:: images/product_sample.png
   :scale: 100 %

   Drill down sample of Product Dimension

Geography Dimension
"""""""""""""""""""

.. figure:: images/geography_dim.png
   :scale: 80 %

   Geography Dimension

The geography dimension describes every transactions places of performance for recipients. It is primary a geographic
dimension in our case study. Each row can be thought of as a location where an award has been made. Because of this we,
can drill down / roll up any geography dimension attribute, such as country, state, city, zip code. These columns
attributes are representing in the geography dimension a simple hierarchy for a single row.

Recipient Dimension
"""""""""""""""""""

.. figure:: images/recipient_dim.png
   :scale: 80 %

   Recipient Dimension

The recipient dimension describes recipient transactions. The dimension contains row for each recipient,
along with descriptive attributes such as street address, state, city etc. We capture only the recipient name,
address information and DUNS number. It is used to establish a business credit file, which is often referenced by
lenders and potential business partners to help predict the reliability and/or financial stability of the company
in question [http://www.dnb.com/duns-number.html]. Recipient geographic attributes have been complicated to
dealing with. We wanted to avoid snowflaking  as we have mentioned in Choose Dimension chapter. In our model
recipients typically have multiple addresses, but every transaction is geocoded on the prime recipient address.
This means we have unique row for each recipient based on his geographic attributes. The street address column
is our natural key to identify each unique recipient.

A sample set of name and location attributes for individual recipient:

+---+----------+--------------------+---------+-------------+-------+--------------------------+-----------+
| 1 | Abbewood | 1002 west 23rd st. | Alabama | Panama City | 32405 | United States of America | 627189244 |
+---+----------+--------------------+---------+-------------+-------+--------------------------+-----------+

Transaction Type Dimension
""""""""""""""""""""""""""

.. figure:: images/transactiontype_dim.png
   :scale: 80 %

   Transaction Type Dimension

The transaction type dimension describes the type of award. It is our smallest dimension consists only of 4 unique row.
The transaction status columns describe the status of the transaction active/inactive. The category columns is our key
attribute in the dimension which describe what kind of transaction is reported by the federal agencies making contract,
grant, loan, and other financial assistance award.

* Contract is an agreement between the federal government and a prime recipient to execute goods and services for a fee.

* Grant is type of federal award that requires an application process and include payments to non-federal entities for
  a defined purpose in which services are not rendered to the federal government.

* Loan is type of federal awards that the borrower will eventually pay back to the government.

* Other Financial Assistance includes direct payments to individuals, insurance payments, and other types of assistance payments.

The transaction description columns provides us descriptive information of types of awards.

Award Dimension
"""""""""""""""

.. figure:: images/award_dim.png
   :scale: 80 %

   Award Dimension

The award dimension is potentially the most important dimension in our schema. The grain of the fact table has been stated
as “awards at the award modification level of detail”. This has been achieved by adding award identifiers from the source
system to identify individual awards: the award_id and award_mod. Together with this two attributes we have achieved to
uniquely identify each fact table rows. Because of this approach, each fact table row represents exactly one award
or award modification, the award dimension and the fact table contain the same number of rows.

**Slowly Changing Dimension**

As our grain dictates, we need to track changes over time. The award modification number is are natural key identifier
carried over from the source system. The award modification number uniquely identify a corresponding entity in our source system.
For example, an individual award is identified by award id in our source system, which uniquely identify a contract or agreement.
With the combination of the natural key award mod from the source system we have achieved to identify each award modification.
The use of surrogate key allows the data warehouse uniquely identify each transaction and respond to changes in the source system.
This allows us to track history changes of awards. With these dimension attributes we have specified a strategy to handle change.
In other words, when an attribute value change in the operational system, we will respond to this change in our dimensional model.
We refer this kind of change as slowly changing dimension. We made the claim earlier about our goals of the data warehouse was to
represent awards correctly. Thats why we have decided to use a technique called SCD Type 2. Using the type 2 approach, when an
agency make a modification to an award (for example restrict the original award or make additional funding) we create a new award
dimension row for the given award to reflect new award amount.


+---+------------+--------------------------------------+
| 1 | 1PIC355199 | c5e18f14-82cf-4e64-b1b0-cd767735e330 |
+---+------------+--------------------------------------+
| 2 | 1PIC355199 | c8130052-2d0b-41f2-9b2b-84f0f3da9d18 |
+---+------------+--------------------------------------+
| 3 | 1PIC355199 | c8c24821-38fe-463f-9fe5-2f3f6db054fc |
+---+------------+--------------------------------------+
| 4 | 1PIC355199 | cc809f09-595b-455a-af92-3498a3c97a90 |
+---+------------+--------------------------------------+
| 5 | Geography  | cd4f8125-a714-4565-8d72-4700f522d54a |
+---+------------+--------------------------------------+



Now we can see why the the award dimension key can’t be the Award ID natural key. We need different surrogate keys
for the same award id. Each surrogate keys identifies a unique transaction that was true for a span of time.
This method accurately track slowly changing dimensions attributes.

Federal Award Transactions
""""""""""""""""""""""""""

As we have moved from the dimensional design, this is how our federal award transaction business model looks like for now.

.. figure:: images/starschema_final.png
   :scale: 80 %

   Star Schema

